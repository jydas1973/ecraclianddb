{
    "service": {
        "create": "<rackname> [cores=<total_cores_num>] [env=nosdi [is_ha=Y|N] [zone=<zone-name>] [dcId=<DatacenterID>] [dcRegion=<DatacenterRegion>] [entitlement_id=<entitlement_id>] [grid 12.2] [ipNet=Y|N] [admin_username=<username>] [appidUser=<appid>] [appidPwd=<paswd>] [instName=<name>] [subtype=<rack subtype>] [isRackReserved=<true|false>] [infra] \nIssue a create service request and wait for completion.",
        "delete": "<exaunit_id> [sse/skip_secure_erase=true/false][dbsystem_id=OCID][keepRackReserved=true/false] [optimizedVMDelete=true/false] [forceVMDelete=true/false] \nIssue a delete service request on the given exaunit and wait for completion.",
        "recreate": "<exaunit_id> [grid_version=12.1/12.2/18]\nDelete exaunit with given id and recreate it in place.",
        "get": "<service_uuid>\nProvide the number of cpus for a cluster for the given service_id if env is oci_exacc. Pull the service information and print out the status parameters otherwise.",
        "update_memory": "<exaunit_id> [json_path=<payloadPath>] [vms=all gb_memory=<MemoryInGb> idemtoken=<idemtoken>] \n Update Memory - Command that allow update memory in exaunit vms giving payload with specific memory for each vm or can update same amount of memory for all vms in exaunit"
    },
    "security": {
        "add_selinuxpolicy": "component=<dom0/cell/domu> policy_name=<name> <policy_file>=<PathToFile> [service=<exacs/adbd/etc>]\nAdd a policy in ecra metadata",
        "list_selinuxpolicy": "component=<dom0/cell/domu>\nList linux policies in ECRA.",
        "dump_selinuxpolicy": "component=<dom0/cell/domu> policy_name=<name> <policy_file>=<PathToFile>\nStore policy in file system.",
        "add_fsencryption": "customer_tenancy_id=<customer_tenancy_id> infra_component=<infra_component>  encryption_mode=<encryption_mode> key_source=<key_source> secret_compartment_id=<secret_compartment_id> vault_id=<vault_id> kms=<kms_id>\nAdd new record in ecs_fs_encryption table to include info in Create Service Flow. If customer_tenancy_id is not provided, it will apply to default tenancy",
        "remove_fsencryption": "customer_tenancy_id=<customer_tenancy_id> Remove FileSystem Encryption info.",
        "list_fsencryption": "rackname=<rackname> List filesystem encryption info components."
    },
    "vm": {
        "start": "<exaunit_id> <vm_name|_all_> [env=gen1|gen2]\nStart the given vm on the given exaunit.\nUse _all_ instead of a vmname to start all vms",
        "stop": "<exaunit_id> <vm_name|_all_> [env=gqn1|gen2]\nStop the given vm on the given exaunit.\nUse _all_ instead of a vmname to stop all vms",
        "restart": "<exaunit_id> <vm_name|_all_> [env=gen1|gen2]\nRestart the given vm on the given exaunit.\nUse _all_ instead of a vmname to restart all vms",
        "status": "<exaunit_id> <vm_name>\nShows if the vm is up",
        "relation": "domu=<admin hostname without domain name> \nShows the information about all the vms that are in the same cluster as the one provided."
    },
    "passwordmanagement": {
        "listuser": " List the weblogic users",
        "getsiv": "Get the vault details",
        "rotate": "username=<username> usertype=<weblogic/db>\n Username and its type",
        "change": "username=<username> usertype=<weblogic/db> newpassword=<newpassword>\n Username, its type and new password",
        "seedsiv": "json_path=<json_path>\n seed siv values in db",
        "updatesivinfo": "Update the vault with new secret name and its secret id.",
        "validatepassword": "'username=<username>\n Validates password in SIV for user if username is given, otherwise for all users'"
    },
    "hardware": {
        "properties": "[racksize] [model]\nGet the hardware information. Optionally racksize and model can be used to filter results.",
        "tenancypropertylist": "\nLists all records from the cloud vnuma tenancy table.",
        "configurefeaturetenancy": "tenancyid=<comma separated values> feature=<ALL|VMBOSS> value=<ENABLED|DISABLED> \n Update configured features in the tenancy(tenancies) selected\n",
        "tenancypropertyput": "tenancy_id=<tenancy_id> [user_group=<user_group>] [cloud_vnuma=<cloud_vnuma>] [jumbo_frames=<jumbo_frames>] [memoryconfig=standard|large|extra_large] [skipresizedg=yes|no] [customids=<true|false> usersdata=<name:uid|name:uid|...> groupsdata=<name:gid|name:gid|...>]\nAdds a new record to the cloud vnuma tenancy records.",
        "tenancypropertydel": "tenancy_id=<tenancy_id>\nRemoves a record from the cloud vnuma tenancy table that matches the tenancy_id."
    },
    "cluster": {
        "details": "[rackname=<name>][exaunitid=<id>][domu=<domuname>][subscriptionid=<id>][dbsid=<dbsid>][resourceocid=<resourceocid>][resourcetype=<adb|cdb|pdb>] [shortresponse=<true|false>] \nGet the cluster details such as cells, domu, dom0, ibswitches and ILOM detals"
    },
    "gcsinfra": {
        "getinfo": "<rackname=<name>> \nGet the infra details such as cells, domu, dom0, ibswitches imageversion & lastreboottime\n"
    },
    "backfill":{
        "caviumip" : "<flatfile=<filepath>>\n Update the cavium IPs for cabinets using the flatfile",
        "adminsmartnics" : "payload=<filepath> [cabinetname=<cabinet name>]\n Backfill admin network info for specific cabinet",
        "update_caviumid" : "<old_id=id1> <new_id=id2>\n Update the cavium ID replacing the old_id with new_id",
        "updatecaviumip" : "<caviumid=caviumid> <caviumip=ip>\n Update the cavium IP for a given cavium ID\n",
        "update_cavium" : "<current_cavium_id=current id> <current_etherface_type=CLIENT|BACKUP> <new_cavium_id=id> <new_cavium_ip=ip> <new_mac=mac>\n This API can be used to update multiple cavium properties like ip, mac, id for a given cavium ID. current_cavium_id is a mandatory input. current_etherface_type is mandatory if new_mac parameter is used.\n",
        "qfabdetails" : "<flatfile=<filepath>>\n Update the canonical QFAB details for the cabinet",
        "fabricexascale" : "<fabricname=<qfab>>\n Updates the exascale IP pool for the given QFAB\n",
        "mvmsubnetinfo" : "flatfile=<filepath> [force=<true>] [cabinetname=<cabinetName> or \nsubnetmask=<mask> gateway=<gateway> domain=<domain> cabinetname=<cabinetName>]\n Update the MVM subnet information on Cabinet or Cabinets.",
        "updatesitegroup": "grouptype=[fabric|cabinet|faultdomain] groupidentifier=<Group name for selected type> sitegroup=<sitegroup_value> \n Update the sitegroup values for CPG on selected group of Cabinets/Fabric or Fault domain, this values should be previously registered",
        "vnic_cabinets": "flatfile=<filepath> Update the admin active/standby vnics"
    },
    "cabinet": {
        "list": "[cabinettype=<cabinet_type>][model=<model>][cabinetstatus=<cabinet_status>][<filter_column>=<value>]\n List all the cabinets that ECRA is managing. If filters are not specified all the cabinets will be retrieved\n Example: cabinet list cabinettype=COMPUTE-ONLY",
        "get": "<cabinetname=cabinetname> \n Get details of the cabinet",
        "ports": "[cabinetid=id] [cabinetname=<cabinet_name>] \n Get ports of the cabinet",
        "update": "<cabinetname> [<key>=<value>]* \n Update certain allowed properties for a given cabinet\n",
        "getxml": "<cabinetname>\n Retreives the xml of the indicated cabinet from the database and saves it in the filesystem.\n",
        "updatexml": "<cabinetname=cabinetname><newxmlpath=filepath>\n Updates the xml of the indicated cabinet wit the file located in the given path.\n",
        "softdeletenode": "<cabinetname=cabinetname><hostname=hostname> [servicetype=<service to which capacity is being given>]\n Retrieves all the information of the node and archives it on ECS_HW_ELASTIC_NODES, then erase it from ECS_HW_NODES \n",
        "getnodes": "<cabinetname=cabinetname><hostname=hostname>\n Get a report of the current status of the nodes, whether they are active or archived.\n",
        "recovernode": "<cabinetname=cabinetname><hostname=[hostname|all] [clustertag=<clustertag>]>\n Recovers all the information of the node/nodes from ECS_HW_ELASTIC_NODES and saves it on ECS_HW_NODES.\nIf clustertag is provided it will be set as clustertag in the node, otherwise ARCHIVED will be set as clustertag. \n",
        "getnodestatusreport": "\n Get a report of all the cabinets, if they are DEDICATED or SHARED and the archived nodes they have, if any.",
        "getnodearchivedreason": "<hostname=hostname>\n Gets the reason a node was archived",
        "updatenodearchivedreason": "<hostname=hostname><reason=reason>\n Updates or sets the reason a node was archived",
        "getnodearchivedextrainfo": "<hostname=hostname>\n Gets all the extra information stored of a archived node",
        "updatenodearchivedextrainfo": "<hostname=hostname><extrainfojsonpath=extrainfojsonpath>\n Updates  the extra information stored of a archived node using the new json file",
        "ingestion": "<xml=path to XML file><flatfile=Path to Flatfile>\n Ingest a new Cabinet or Add new nodes to present Cabinet (only for Elastic Flex Cabinets)",
        "modelsubtype": {
            "convert": "modelsubtype=[LARGE | EXTRALARGE | EF | Z | STANDARD] nodetype=[COMPUTE | CELL] model=[X11M | X10M-2 | X9M-2] {[cabinetname=[string name] AND amount=[1 - n]] OR hostnames=[comma splitted]}\n This command will convert the nodes to the required modelsubtype and set its state in HW_REPAIR to avoid be used by ECRA during the upgrade, you can define the exact nodes, just the amount needed no matter the cabinet, or the amount an the cabinet where need to be converted",
            "convertlarge": " {nodetype=[COMPUTE | CELL] AND model=[X11M | X10M-2 | X9M-2] AND amount=[1 - n]} OR {cabinetname=[string name] AND amount=[1 - n]} OR {hostnames=[comma splitted]}\n Wrapper for convert command, setting LARGE as modelsubtype",
            "convertextralarge": "{nodetype=[COMPUTE | CELL] AND model=[X11M | X10M-2 | X9M-2] AND amount=[1 - n]} OR {cabinetname=[string name] AND amount=[1 - n]} OR {hostnames=[comma splitted]}\n Wrapper for convert command, setting EXTRALARGE as modelsubtype",
            "convertstandard": "{nodetype=[COMPUTE | CELL] AND model=[X11M | X10M-2 | X9M-2] AND amount=[1 - n]} OR {cabinetname=[string name] AND amount=[1 - n]} OR hostnames=[comma splitted]}\n Wrapper for convert command, setting STANDARD as modelsubtype",
            "getreport": "modelsubtype=[LARGE | EXTRALARGE | EF | Z | STANDARD] nodetype=[COMPUTE | CELL] model=[X11M | X10M-2 | X9M-2] {cabinetname=[string name]}\n Generates a report with the available cabinets and nodes to convert",
            "releasenodes": "{requestid=[UUID] OR cabinetname=[string name] OR hostnames=[comma splitted]\n After the Nodes are updated by ops, this command will release the nodes to be used by ECRA, by setting the node state in FREE status "
        },
        "composexml": "<cabinetname=cabinetname> [force=true/false]\n Generates the xml for the cabinet and stores it in cabinet registry.\nIf force option is set to true the xml will be generated and will replace the existing xml",
        "domu": {
            "get": "<dom0> \n Get all the MVM DomUs infomation related to the provided Dom0",
            "update": "<hostname> [admin_nat_ip=<NAT IP>] [db_client_mac=<db_client_mac>] [db_backup_mac=<db_backup_mac>] [admin_vlan_tag=<admin_vlan_tag>]\n Update the specified domu"
        }
    },
    "bonding": {
        "getInfo": "[rackname=<name>||hostname=<name>] \n Provide bonding related info associated with the rack or hostname from ecs_bonding table",
        "setupMonitoringBond": "json_path=<bondinginfo> | hostname=<hostname> [skipbonding=<true/false>]\n Sets up the monitoring bond or updates it.\nIf hostname is provided ECRA will retrieve the information from bonding tables, this should be only used if the bonding was previously set in ECRA (rebonding). Use the skipbonding flag to only update the adming monitor file for non eth0 nodes.",
        "deleteMonitoringBond": "json_path=<bondinginfo> \n Deletes the monitoring bond\n",
        "getpayload": "[rackname=<name>||hostname=<name>]\n Get the DBCP payload for bonding from ecra_clobs table\n",
        "retrysetup": "rackname=<name> \n Retry setupMonitoringBond API \n",
        "runprecheck": "json_path=<json file, with path, of bonding precheck request>\n Run precheck for bonding migration\n",
        "smartnicaction": "nodeid=<Compute Node's Oracle host name> actionjson=<json file, with path, for the SmartNIC's operation from DOM0 bond_utils.py and the operation's parameters.\n Please make sure to follow the syntax defined for the ecra endpoint /bonding/[nodeid]/ports/smartNICAction>",
        "migratevlan": "json_path=<vlaninfo> \n Migrates the Vlans\n",
        "networkupdate": "json_path=<networkinfo>\n Migrates the networking information\n",
        "monitorswitch": "json_path=<monitorswitch>\n Enable/Disable bond monitor\n",
        "rpmupdate": "rackname=<clustername>\n Updates the bond monitor rpm on the given cluster\nhostnames=<hostname1,hostnameN> Will update single or multiple dom0\n",
        "rpmupdateall": "[selectnodes=<FREEONLY|PROVISIONONLY|ALL> mode=batch]\n Updates the bond monitor rpm on all nodes depending on the selectnodes filter\n[mode=batch|normal] If no mode is specified then by default is normal, batch is parallel mode.\nselectnodes=FREEONLY|PROVISIONEDONLY|ALL Filter nodes by this flag (only available with batch mode)",
        "getelasticcabinets": "\nGet all the elastic cabinets where nodes are bonded\n",
        "addcustomvip": "<json_path=file>\n Registers custom vips\n",
        "deletecustomvip": "<json_path=file>\n De-registers custom vips\n",
        "linkfailover": "<components=name> [<newactive=name>]\n Helps to failover the link on the interfaces on the indicated compute node\n",
        "consistencycheck": "rackname=<rack name> [hostname=<dom0 hostname>\n Execute the Bonding consistency check over the provided rack. If hostname provided, consistencycheck will only run on that node",
        "restartmonitor": "component=<COMPONENT>\n Restarts the bond monitor for the provided component. The component could be exadataInfrastuctureId, rackname (provisioned or not), or hostname.",
        "status": "component=<COMPONENT> [id=<operation_id>]\nCheck the status for bondmonitor & networking conectivity. The component could be exadataInfrastructureId, rackname (provisioned or not), or hostname. After using the component param you should fetch the data using id=<operation_id>"
    },
    "kvmroce": {
        "allocateComputeVlan": "<fabric_name=name><rackname=name>[number_of_vlans=number][ListOfVlans=comma separated ranges]\n  Allocate a compute VLAN for KVM RoCE or try to reserve the provided list of vlans\n",
        "allocateStorageVlan": "<fabric_name=name><rackname=name>[number_of_vlans=number][ListOfVlans=comma separated ranges]\n  Allocate a storage VLAN for KVM RoCE or try to reserve the provided list of vlans\n",
        "getComputeVlan": "<rackname=name>\n  Get the compute VLAN allocated for this rack\n",
        "getStorageVlan": "<rackname=name>\n  Get the storage VLAN allocated for this rack\n",
        "freeComputeVlan": "<fabric_name=name><[vlan_id=id] | [ListOfVlans=comma separated ranges]>\n  Free the allocated compute VLAN or the provided list of vlans\n",
        "freeStorageVlan": "<fabric_name=name><[vlan_id=id] | [ListOfVlans=comma separated ranges]>\n  Free the allocated storage VLAN or the provided list of vlans\n",
        "allocateComputeIP": "<fabric_name=name><rackname=name> <shape=<quarter|half|full|double> | <shape=elastic> <number_of_nodes=<no>>> [ListOfIps=<comma separated IPs>] [ListOfNodes=<comma separated hosts>] [generateNodeNames=true/false] [exacompute=true/false]\n  Allocate clusterInterconnect and storage IPs for the compute for KVM RoCE\n",
        "allocateStorageIP": "<fabric_name=name><rackname=name> <shape=<quarter|half|full|double> | <shape=elastic> <number_of_nodes=<no>>> [ListOfIps=<comma separated IPs>] [ListOfNodes=<comma separated hosts>] [generateNodeNames=true/false]\n  Allocate storage IPs for the storage nodes for KVM RoCE\n",
        "allocateExascaleIP": "<fabric_name=name><rackname=name> <shape=<elastic> | <number_of_nodes=<no>>> [ListOfIps=<comma separated IPs>] [ListOfNodes=<comma separated hosts>] [generateNodeNames=true/false]\n  Allocate exascale IPs for the storage nodes for KVM RoCE\n",
        "getComputeIP": "<rackname=name>\n  Get the cluster interconnect IPs allocated for this rack\n",
        "getStorageIP": "<rackname=name>\n  Get the storage IPs allocated for this rack\n",
        "getExascaleIP": "<rackname=name>\n  Get the exascale IPs allocated for this rack\n",
        "freeComputeIP": "<fabric_name=name><<rackname=name> | <ListOfIps=<comma separated IPs>>\n  Free the allocated compute IPs associated with a rack, or the given list of IPs\n",
        "freeStorageIP": "<fabric_name=name><<rackname=name> | <ListOfIps=<comma separated IPs>>\n  Free the allocated storage IPs associated with a rack, or the given list of IPs\n",
        "freeExascaleIP": "<fabric_name=name><<rackname=name> | <ListOfIps=<comma separated IPs>>\n  Free the allocated exascale IPs associated with a rack, or the given list of IPs\n",
        "addFabric": "<fabric_name=name> [fabric_type=<type>] \n  Add a fabric in this AD \n",
        "deleteFabric": "<fabric_name=name> \n  Delete a fabric in this AD\n",
        "listFabric": "[verbose=yes|no] \n  Lists all the fabric names present in this ECRA along with cabinet info\n",
        "getAllFabric": "\nProvides high level information about all fabrics in an AD\n",
        "getAllCabinets": "\nProvides high level information about all cabinets in an AD\n",
        "getAllNodes": "\nProvides high level information about all nodes in an AD\n",
        "getCabinets": "<fabric_name=name> \n Provides information about the cabinets in a given fabric\n",
        "getCabinetNodes": "<fabric_name=name> <cabinet_name> \n Provides information about nodes of a cabinet on a Roce fabric\n",
        "runsanitycheck": "<json_path=<file location>> | operationtype=<ADD_COMPUTE|ADD_CELL|CEI_PRECHECK|INVENTORY_RESERVE|CEI_RESERVE|DELETE_CELL> rackname=<rackname> exadataInfrastructureId=<infra ocid> [elasticnodes=<commma separated hostnames>] [serverscount=num of servers required(Used for delete cell flow)]\n Run the sanity check API\n",
        "getsanityresults": "<idemtoken=<token>> \n Get the sanity check resuls from exacloud\n",
        "listfaultdomains": "\n Gets a list of all fault domains and fabrics within them\n",
        "isexascalepoolcreated": "\n<fabricname=name> \n Returns if exascale pool has been already created or not\n",
        "updatefaultdomain": "<fabric_name=fabricname> <faultdomain=faultdomain> \n Updates fault domain of given fabric"
    },
    "exawatcher": {
        "getlog": "<rackname=rack>[fromtime=<time to collect logs from>][totime=<endtime>][targets=<hostname1,hostname2][filter=NetStat etc][targettypes=<dom0|cell|all>]\n Get the exawatcher logs",
        "listlog": "<rackname=rack>[retrieval-history=value]\n Retrieves the log location of the past runs, 10 is the default value"
    },
    "higgs": {
        "createNetwork": "[<subscriptionid>] <name=network-name> <ipAddressPrefix=<CIDR>> [<admin_username=uname>] [<appidUser=appid>] [<appidPwd=secret>] [network_username=username]\n Create a customer network with the given name and CIDR block.",
        "getNetwork": "[<subscriptionid>] <networkname=name> [<admin_username=uname>] [<appidUser=appid>] [<appidPwd=secret>] [network_username=username]\n Get the network details for a given network.",
        "getAllNetwork": "[<subscriptionid>] [<admin_username=uname>] [<appidUser=appid>] [<appidPwd=secret>] [network_username=username]\n Get all the networks created for a given subscription.",
        "deleteNetwork": "[<subscriptionid>] <networkname=name> [<admin_username=uname>] [<appidUser=appid>] [<appidPwd=secret>] [network_username=username]\n Delete the specified network.",
        "deleteResources": "[<subscriptionid>] <rackname=<clustername>> [<clusterId=<clusterId>>] [<admin_username=uname>] [<appidUser=appid>] [<appidPwd=secret>] <instName=instancename> \n Delete all the higgs resources created for the given cluster.",
        "install": "<rackname=<clustername>> <bond0_ips=comma-separated-IPList> <bond0_mask=mask> <bond0_gw=gateway>\n Setup higgs environment on given cluster.",
        "register": "<rackname=<clustername>> [rack_type=multivm|singlevm] <bond0_ips=comma-separated-IPList> <bond0_mask=mask> <bond0_gw=gateway>\n Register higgs information in ECRA(No installing). <rack_type> defaults to singlevm if not specified.",
        "deregister": "<rackname=<clustername>> \n Deregister higgs information from ECRA.",
        "createNatvips": "<exaunitId> <target=node_vips|scan_vips|client_nat_ip|all|client|backup_nat_ip> \n Generate Nat VIPs for specified RackName.",
        "listNatvips": "<exaunitId> <target=node_vips|scan_vips|client_nat_ip|all|client|backup_nat_ip> \n List current Nat VIPs for specified RackName.",
        "deleteNatvips": "<exaunitId> <target=node_vips|scan_vips|client_nat_ip|all|client|backup_nat_ip> \n Delete current Nat VIPs for specified RackName.",
        "audit": "bond0_ip=<registeredIP> [debug=true] \"<command>\"\nExecute whitelisted audit commands on Higgs compute nodes.",
        "addIbsubnet": "<CIDR_Block> i.e 192.68.5/24\nAdd an ibsubnet to validate and do not interfere with higgs client and backup networks.",
        "deleteIbsubnet": "<CIDR_Block>\nDeletes from higgs ecra configuration the specified CIDR Block.",
        "createSecRule": "<subscriptionID> admin_username=<administrator> appidUser=<appid> appidPwd=<secret> [name=<secure rule name> acl=<acl name> secProtocols=<list of protocols> [srcVnicSet=<source vnic set>] [dstVnicSet=<destination vnic set>] [srcIpAddressPrefixSets=<list of srcIpAddressPrefixSets>] [dstIpAddressPrefixSets=<list of dstIpAddressPrefixSets>] [flowDirection=<ingress|egress>] | json_path=<json file>] \n Create a secure rule.",
        "getSecRule": "<subscriptionID> admin_username=<administrator> appidUser=<appid> appidPwd=<secret> name=<secure rule name> \n Get the information of the given secure rule.",
        "deleteSecRule": "<subscriptionID> admin_username=<administrator> appidUser=<appid> appidPwd=<secret> name=<secure rule name> \n Delete a secure rule.",
        "getResources": "<subscriptionID> admin_username=<administrator> appidUser=<appid> appidPwd=<secret> resource_type=<resource Type> \n Get all the resources of the given type.",
        "removeResource": "<subscriptionID> admin_username=<administrator> appidUser=<appid> appidPwd=<secret> name=<resource uri> \n Delete a higgs resource.",
        "createSecProtocol": "<subscriptionID> admin_username=<administrator> appidUser=<appid> appidPwd=<secret> rack=<rack> ipProtocol=<protocl> dstPortSet=<port> \n Create a secure protocol."
    },
    "atp": {
        "getDetails": "<dbSystemId=ID> \n Get ATP network details for a given a dbSystemID",
        "getNetwork": "<dbSystemId=ID> [shape=observer]\n Get the subnet OCID for a given dbSystemID for ATP network.",
        "registerVnic": "<json_path=filepath of vnic info> [idemtoken=<idemtoken>]\n Registers the VNIC information in ECRA.",
        "getVnicDetails": "<dbSystemId=ID> \n Get the vnic details for a given dbSystemId",
        "registerVcnDetails": "<json_path=filepath of OM VCN details>\n Registers the OM VCN information in ECRA.",
        "bootstrap": "<json_path= filepath>\n Sets up some ecs_properties needed for ATP as part of bootstrap",
        "createOciUrlMap": "<json_path= filepath>\n Sets up realm & domains in ECRA ",
        "getOciUrlMap": "<realm=realmname>\n Lists the OCI realm & domain present in ECRA for the given realm ",
        "getOciUrlMapAll": "\nLists all the OCI realm & domains that are present in ECRA ",
        "registerAuth": "<tenancy=omtenancy ocid>[region=name][user=omuserocid][fingerprint=value] [om_compartment_ocid=compartmentid> [profile=<name>][json_path=<path>] [passphrase=<passphrase>] [orcl_client=<True|False>]\n Registers authentication details for OCI with ECRA.",
        "registerAdminIdentity": "<tenancy_ocid=tenancy ocid> [region=<region name>] [user_ocid=<user ocid>] [fingerprint=<value>] [compartment_ocid=<compartment ocid>] [profile=<name>][json_path=<path>] [private_key_path=<private pem key path>] [passphrase=<passphrase>] [loadbalancer_ocid=<load balancer ocid>] [domain=<region based domain url ex.'r1.oracleiaas.com'>] [cert_path=<path to cert bundle to perform verification against federation endpoint>]\n Registers admin tenancy authentication details for OCI with ECRA.",
        "createNetwork": "<custTenantId=OCID> <dbSystemId=ID> <shape=quarter/half/full/double/observer/elastic> <region=regionname> <ad=Availability Domain> <rackname=name> [idemtoken=<idemtoken>]\n Create an ATP network.",
        "deleteNetwork": "<dbSystemId=ID> <optimizedDelete=true/false> \n Delete an ATP network.",
        "registerSubnet": "<vcnCIDR=omvcncidr> [adName=Name of the ad] \nCreates the subnet pool cidr blocks and registers them to ECRA",
        "getPartnerSubnet": "<subnetOcid=OCID> \n Get the Subnet details such as CIDR and DNS label for the given OCID",
        "setupDGNetwork": "<json_path=path> <targetType=primary/standby/observer> <slaAvailabilityType=HA/DR> \n Setup the dataguard security rules between primary,standby and observer nodes depending on the targetType",
        "deleteDGNetworkRules": "<json_path=path> <targetType=primary/standby/observer> \n Delete the dataguard security rules between primary,standby and observer nodes depending on the targetType",
        "addProperty": "<value=propValue> <type=propType> [name=propName]",
        "getProperty": "[name=propName] [value=propValue] [type=propType]",
        "deleteProperty": "<value=propValue> <type=propType>",
        "listCustomerTenancy": "[dbsystem_id=value] \nLists all records from the customer tenancy database table.",
        "putCustomerTenancy": "<dbsystem_id=value> [cloud_account_id=value], [tenancy_name=value], [tenancy_ocid=value], [rackname=value], [compartment_id=value] [ceiocid=infraid] [vmclusterocid=vmclusterocid] \nAdd a record to the customer tenancy database table. If a record with the same dbsystem_id/vmclusterocid  exists then it is only modified.",
        "deleteCustomerTenancy": "<dbsystem_id=value>\nDeletes a record from the customer tenancy database table.",
        "createOracleClientSubnet": "<shape=quarter/half/full/double/observer> [rackname=name]\n Create Client Subnet Network for PreProvisioing",
        "giServiceStop": "<rackname=name> \n GI Service Stop for ATP Preprovisioning",
        "updateNetMetadata": "[rackname=<name>] [dev=<true|false>] <json_path=path> <addCluster_json_path=path of add cluster>\n Updates the ECRA metadata information",
        "launchObserver": "<dbSystemId=ID> <custTenantId=OCID> <user_data=base64encodeddata>\n Launch an observer instance for the dbsystemId and a given tenancy",
        "deleteObserver": "<custTenantId=OCID><dbSystemId=ocid>\n Deletes an observer instance",
        "getObserverDetails": "<dbSystemId=ID>\n Get the observer instance details for ATP network.",
        "fetchObserverKeys": "<dbSystemId=ID>\n Fetch the SSH keys for the observer instance",
        "createPreprovDbSystem": "<rackname=name>\n Create Preprovision DbSystem with ORCL Client",
        "getPreprovScheduler": "\n Get pre provisioning scheduler details.",
        "listScheduledRacks": "\n List racks scheduled for pre provisioning.",
        "getScheduledRackPreprovJobs": "<rackname=name>\n Get pre provisioning jobs associated with given rack.",
        "startPreprovScheduler": "[payload=<payload file path>]\n Start pre provisioning scheduler. If payload file is not provided, scheduler will be started with default payload present at <ecracli src>/tmpl/atp_preprov_scheduler.json. Input payload file must be based on <ecracli src>/tmpl/atp_preprov_scheduler.json.",
        "shutdownPreprovScheduler": "\n Shutdown pre provisioning scheduler.",
        "terminateObserver": "<custTenantId=OCID>\n Terminates an observer instance",
        "startObserver": "<custTenantId=OCID>\n Starts an observer compute instance, used during LCM operations",
        "stopObserver": "<custTenantId=OCID>\n Stops an observer compute instance, used during LCM operations",
        "restartObserver": "<custTenantId=OCID>\n Soft resets an observer compute instance, used during LCM operations",
        "reconfigService": "<rackname=name> <reconfig_payload=payload_path> \nReconfig a service.",
        "deletePreprovOracleClientVCN": "<rackname=name>\n Delete an oracle client vcn.",
        "deletePreprovOracleDbSystem": "<rackname=name>\n Delete an oracle dbsystem along with any oracle client vcn created during dbsystem preprovision.",
        "ingestTerraformData": "<json_path=Path to terraform json> <oss_filename=OSS filename to perform terraform ingestion> [resetMgmtSubnetPool=true/false] [resetMgmtVcn=true/false] \n Perform terraform ingestion given a json file in filesystem or OSS. If resetMgmtSubnetPool is passed as true, subnets with cidr_block equal to that of mgmt_vcn section in terraform json will be recreated only if 'ATP_OM_VCNID' property contains a different value as the id from mgmt_vcn section. resetMgmtVcn option cleans up old mgmt vcn data",
        "configDom0Rule": "<rackname=name> <port=portNum> <action=add/remove> [source=sourceIp] [target=targetIp] [protocol=ssh/udp/tcp/http/icmp] Sets a VCN config rule for a given client.",
        "atpvmrollback": "<exaunit_id> \n Performs a rollback on the previous backup vm from preprov dbSystem.",
        "configDomURules": "<rackname> <newCIDRs comma separated>\n This command update CIDRs value for given rack",
        "getPreprovMetrics": "<rackname> \n get a preprov metrics based on the provided rackname",
        "createPreprovExadataInfrastructure": "<rackname> \n Create a preprov Exadata Infrastructure based on the provided model",
        "createPreProvVMCluster": "<> <>\n This command will create preprov vmcluster based on exadatainfrastructureOcid and NetworkOcid"
    },
    "exaunit": {
        "info": "<exaunit_id> \nGet network information from rack xml for the given exaunit.",
        "logs": "<exaunit_id> [dest=destination_folder] [dbName=<dbname>] [vmName=<vmname>]\nExtract DB logs from provisioned environment.",
        "cores": "<exaunit_id>\nGet cores allocation info for the given exaunit.",
        "dbs": "<exaunit_id> \nList dbSIDs of created databases for the given exaunit.",
        "detail": "<exaunit_id> \nGet the detailed parameters for the given exaunit.",
        "detail_update": "<exaunit_id> \nUpdate a few elements in exaunit details info (grid_version,customer_name,initial_cores, image_version, reserved_cores, reserved_memory, filesystem, gb_memory, tb_storage, gb_storage, gb_ohsize, atp, jumbo_frames, monitoring_enabled, total_cores, fsconfig, ecpufactor).",
        "vms": "<exaunit_id> \nGet the vm names for the given exaunit.",
        "domukeys": "<exaunit_id> [user=<username>] [ttl=<ttl(in seconds)>]\n Get the domukeys of all the vm's for a cluster.",
        "resize": "<service_uuid> <cores> \nResize the given service with the given number of cores.",
        "get": "<exaunit_id> \nPull the exaunit information and print out the status parameters.",
        "drop": "<exaunit_id> \nDrop the exaunit record as if executing the post action of delete_service. This will delete all the related info about this exaunit in ecra and release resources occupied by this exaunit.",
        "asmrebalance": "<exaunitid> rebalancepower=<rebalance power>\nUpdate the ASM rebalance power value, this value is used during the rebalance process when an elastic cell is added to the disk group.",
        "hasopr": "<exaunit_id> \nRetrieve currently ongoing operation for the given exaunit, if any",
        "list": "<dom0_bonding=Y/N>\n Retrieve the racks that are provisioned using dom0_bonding",
        "resizefs": "<exaunit_id><filesystem='rootfs|u01|var|grid|tmp'> <extra_size='#GB'> [listofnodes=<FDQNs>] [validate_max_size=<true|false>]\n This command call Endpoint in Exacloud to allow resize file system.",
        "addcompute": "<exaunit_id> jsonPayload=<payloadPath> hostname=<hostname>\n Add a set of compute(s) to this exaunit with the given payload.",
        "addcell": "<exaunit_id> jsonPayload=<payloadPath>\n Add a set of cell(s) to this exaunit with the given payload.",
        "deletecell": "<exaunit_id> cellnodes=<cells separated by ,>\n Delete a set of cell(s) from this exaunit",
        "deletecompute": "<exaunit_id> [jsonPayload=<payloadPath>] [hostnames=hostnames] [force=true/false]\n Deletes a set of compute(s) to this exaunit with the given payload or hostnames.",
        "reshape": "<exaunitId> <exaOcid> [reshapeOps] [env=gen2] [infra=ocimvm] \n reshape service - reshapeOps can be cores or memory or storageTb or ohomeGb for given exaunit id & exaOcid",
        "monitoring": "[[exaunit=]<exaunit>] \n return a set of ECRA API responses: exaunit_detail, exaunit_dbs and cluster_details",
        "getcspayload": "<exaunit_id> payloadtype=<ecra|exacloud>\n returns create service payload of ecra or exacloud",
        "reshapeprecheck": "jsonPayload=<jsonPayload> \nRun mvm reshape precheck for memory, local storage or exadata storage",
        "elasticnodeprecheck": "jsonPayload=<jsonPayload> \nRun elastic node precheck for add cell or add compute",
        "getelasticnodeprecheck": "idemtoken=<jsonPayload> \nGet exacloud output for previous run of add cell precheck or add compute precheck",
        "suspend": "<exaunit_id>\nSuspend all the vms in the exaunit.",
        "generatecspayload": "[workflowid=<workflow uuid>] [payload=<base cspayload> filename=<filename for new cspayload>].\nGenerates the create service payload for exacloud.\nIf workflowid is provided the generated payload will be saved in the POD repo using the same name as the one generated in the workflow.\nIf payload and filename are provided the generated payload will be stored in the POD repo with the specified filename.",
        "updatetimezone": "exaunitid=<exaunitid> timezone=<time zone>.\nUpdates the time zone in the xml for the exaunit.",
        "getdginfo": "exaunitid=<exaunitid>\nDisplay the disk group info for the given exaunit.",
        "getallforinfra": "exaOcid=<exaOcid> env=ocimvm\n Get list of all provisioned exaunits on a given infra with resources details",
        "securevms": "exaunitid=<exaunitId> [opcrequestid=<opcrequestid>] [idemtoken=<idemtoken>] [payload=<json_path>] \n Erase the ssh keys of every vm of the indicated exaunitId for its protection.",
        "getoperations": "exaunitid=<exaunitId>\n Retrieves the operations that are running in the cluster",
        "updatentpdns": "exaunitid=<exaunitId>\n Update the NTP and DNS details for the exaunit in xml.",
        "rotatekeys": "[exaunitid=<exaunitId>] [rackname=<rackname>] [idemtoken=<idemtoken>] [force=true/false]\n Rotate the keys of the given exaunit/rackname, the exaunitid or rackname should be provided, only one of them. If force=true  then the operation will be performed even if there are ongoing operations in the cluster",
        "getcomputesizes": "<exaunit_id> [dom0s=<dom0_1>,<dom0_2>,...,<dom0_n>] [ignorexml=<true|false>] [wait=<true|false]>\nGet the disk usage of each dom0 from a given exaunit. If a list of dom0s is specified with the 'dom0s' parameter, they will be listed in addition to the dom0s from the exaunit's XML (if found). If ignorexml=true, only the dom0s specified with the 'dom0s' parameter will be listed. If wait is enabled, the output will be pretty-formatted and ECRACLI will wait for the async endpoint to return 200.",
        "collectvmcore": "exaunitid=<exaunitId> vmName=<vm_name>\nCalls Exacloud to start the collection of the VM core logs for a given VM. Once in status 200, the status details will contain the OSS URL where the logs can be found."
    },
    "db": {
        "create": "<exaunit_id> <dbVersion=11.2.0.4/12.1.0.2/12.2.0.1><dbTemplate=oltp/dw>[clusterID][backupDest][dbSID=<dbSID>][dbUniqueName=<dbUniqueName>][ocid=ocid][idemtoken=<idemtoken>] \nIssue a create db request on the given exaunit and wait for completion.",
        "recreate": "<exaunit_id> dbSID=<dbSID>",
        "info": "<exaunit_id> dbSID=<dbSID> \nIssue a fetch db info request on the given exaunit and wait for completion.",
        "info_all": "<exaunit_id> \nIssue a fetch db info request on the given exaunit and wait for completion. This will return a list of all the cdbs associated to the given exaunit.",
        "delete": "<exaunit_id> [dbSID=<dbSID>] \nIssue a delete db request on the given exaunit and wait for completion.",
        "create_starter": "<exaunit_id> <dbVersion 11.2.0.4/12.1.0.2/12.2.0.1><dbTemplate=oltp/dw>[dbSID=<dbSID>][dbUniqueName=<dbUniqueName>][ocid=ocid] [deferKeyDeletion=<true|false>][idemtoken=<idemtoken>]\nIssue a create starter db request on the old endpoint /ecra/endpoint/databases.",
        "rollback_starter": "<exaunit_id> dbSID=<dbSID> \nIssue a rollback starter db request on the old endpoint /ecra/endpoint/databases.",
        "register": "<exaunit_id> dbSID=<dbSID> [dbUniqueName|dbType|dbVersion|clusterID|tenantID|backupDest|timezone|status|ocid]=<value> \nRegister additional db created outside cloud automation, in ecra db.",
        "deregister": "<exaunit_id> dbSID=<dbSID> \nDe-register db from ECRA metadata.",
        "registered_info": "<exaunit_id> <dbSID=<dbSID> \nShow db information from ECRA metadata.",
        "backup": [
            "backup list    <exaunit> <dbname>",
            "backup start   <exaunit> <dbname>",
            "backup start   <exaunit> <dbname> content=full",
            "backup delete  <exaunit> <dbname> tag=<tagname>",
            "backup update wallet <exaunit> <dbname>"
        ],
        "recover": "<exaunit> <dbname> to=<option>\noptions: latest, JAAS20160805T2121, 50100, 31-JAN-2014 14:50:07",
        "patch": [
            "<exaunit> <action> <dbname> patchid=#####",
            "The dbpatch command is designed to apply, precheck and rollback the grid/db patches for ecs databases.",
            "Note: supported actions list, apply, rollback, precheck",
            "-> Examples:",
            "*) dbpatch <exaunit> list <dbname>",
            "*) dbpatch <exaunit> apply <dbname> patchid=#####"
        ]
    },
    "pdb": {
        "register": "exaunit_id=<exaunit_id> cdb_sid=<Container Database> pdb_name=<Pluggable database> ocid=<ocid> \nRegister a new pluggable databse in ECRA db.",
        "deregister": "exaunit_id=<exaunit_id> cdb_sid=<Container Database> pdb_name=<Pluggable database> \nDe-register the pluggable database from ECRA db",
        "info": "exaunit_id=<exaunit_id> cdb_sid=<Container Database> [pdb_name=<Pluggable database>] \nFetch the pluggable database metadata."
    },
    "em": {
        "reregister": "Usage : <exaunit_id>|<rack_name>|all [cdb_sid=<Container Database>] [pdb_name=<Pluggable database>]  \nRe-register resources with EM.\nArguments:exaunit_id : The id such as 1, 2 etc or rackname such as scas07adm0506clu2 or all (mandatory)\ncdb_sid : database (CDB) SID such as POD1 (optional) \npdb_name : pluggable database name such as PDB1 (optional)\nExamples:ecra> em reregister scas07adm0506clu2\n* PUT http://slc16cya.us.oracle.com:9001/ecra/endpoint/em/2*\n {\"status\": 200, \"status-detail\": \"success\", \"op\": \"exaunit_em_rereg_clu\"}",
        "list": "Usage : [<exaunit_id>|<rack_name>] [cdb_sid=<Container Database>] [pdb_name=<Pluggable database>]  \nGet EM metadata table information.\nArguments:exaunit_id : The id such as 1, 2 etc or rackname such as scas07adm0506clu2(optional)\ncdb_sid : database (CDB) SID such as POD1 (optional) \npdb_name : pluggable database name such as PDB1 (optional)\nExamples:ecra> em list scas07adm0506clu2\n* GET http://slc16cya.us.oracle.com:9001/ecra/endpoint/em/2*\n {\"status\": 200, \"status-detail\": \"success\", \"op\": \"exaunit_em_info_clu\", \"em_resource\": \"[{\"db_sid\": \"NA\",\"em_state\": \"created\",\"em_type\": \"cluster\",\"exaunit_id\": 2,\"reg_attempt\": 4,\"resource_info1\": \"NoVal\"}]\"}",
        "setup": "Usage : [property_json] \nSetup EM using DIAG_EM properties from input json.\nArguments:input_property_json (optional) : JSON containing the following property values : \nDIAG_EM_HOST\nDIAG_EM_PASSWD\nDIAG_EM_USER\nDIAG_EM_PORT\nDIAG_EM_AGENT_HOST\nDIAG_EM_AGENT_PORT",
        "update": "Usage : <row_info.json>\n Updating values of a row of the EM metadata table.\n Arguments: row_info.json (mandatory): new values of the row to be updated.\nem_type,       (mandatory)\nexaunit_id,    (mandatory)\ndb_sid,        (mandatory if em_type = 'database'/'pdb')\nem_state,      (optional)\nreg_attempt,   (optional)\nresource_info1,(mandatory if em_type = 'pdb')\nresource_info2,(optional)\nem_agent       (optional)",
        "add": "Usage : <row_info.json>\n Adding a row in the EM metadata table.\n Arguments: row_info.json (mandatory): new value of the row to be added.\nem_type,       (mandatory)\nexaunit_id,    (mandatory)\ndb_sid,        (mandatory if em_type = 'database'/'pdb')\nem_state,      (mandatory)\nreg_attempt,   (mandatory)\nresource_info1,(mandatory if em_type = 'pdb')\nresource_info2,(optional)\nem_agent       (optional)",
        "delete": "Usage : <row_info.json>\n Deleting rows from the EM metadata table.\n Arguments: row_info.json (mandatory): Info for the row to be deleted.\nexaunit_id,    (mandatory)\ndb_sid,        (mandatory if em_type = 'database'/'pdb'\nresource_info1,(mandatory if em_type = 'pdb')",
        "disable": "Usage : em disable\n Disable em recurrent job for registration/de-registration of resources\n Arguments: None",
        "enable": "Usage : em enable\n Enable em recurrent job for registration/de-registration of resources\n Arguments: None"
    },
    "rack": {
        "register": "<rackname> | rackname=<rack name> | rackxmlpath=<xml file path> [racksize=<full | half | quarter | eighth | elastic>]\nRegister a rack in ecra with the given rack name or xml file path.",
        "deregister": "<rack_name> | rackname=<rack name> | rackxmlpath=<xml file path> \nDeregister the rack in ecra with the given name or xml file path.",
        "update": "<rack_name> [options] \nUpdate the given rack with specified options.",
        "get": "[options] \nQuery all the racks that fulfill input option requirements.",
        "compose": "<rackname> Request to exacloud for a new Rack XML. If it is successful the XML contents are saved in the rack entry and the harware nodes status are set to ALLOCATED.\nNote: If the rack has allocated nodes and the original xml field is not empty, then the generated xml will be saved in the updated_xml field, otherwise it will be saved in the original field.",
        "reserve": "<rack_name> \nReserve the rack in ecra with the given name.",
        "release": "<rack_name> \nRelease the rack in ecra with the given name.",
        "drop": "rackname=<rack_name> \nThis will delete all rack related records and the rack itself, all hw nodes related will be restored",
        "fetchatp": "<racksize=<size>> [model=<model>] Gets a rack from ATP pool to ExaCS pool if its available",
        "getxml": "<rackname> Retrieve the rack's original and updated xml.",
        "unlock": "<rackname> Unlock rack's cells and dom0 using ilom.",
        "nodes": " rackname=<rackname> Retrieve hosts association for this rack.(Computes and cells)",
        "exaid": "<[exaunitid=id] | [dbSystem_id=ocid]> \n Return the exaunitId or dbsytemId association",
        "ports": " rackname=<rackname> Retrieves cavium information | type=elastic return all the elastic reserved info | type=computeonly return compute-only cabinet info | type=infrastructureall Get all nodes that belong to the infra based on the rack slot | <nodeList> List of nodes comma-separated, could be oracleHostames or dbaliases(In case infrastructureall) if not, should be oracleHostnames",
        "xml patch": "<rackname> timezone=<tz> \n Call exacloud to patch the rack XML",
        "update_selinux_policy": "rack update_selinux_policy rackname=<RACKNAME> cabinetname=<CABINETNAME> component=<COMPONENT> [targetComponentName=<LISTOFCOMPONENTS> policyVersion=<POLICYVERSION>]\n Updates the policy in ECRA table and send request to EC in order to update the information in the components ",
        "list_selinux_policy": "rack list_selinux_policy rackname=<RACKNAME> \n Get all the policies for the given rackname",
        "update_custom_selinux_policy": "rack update_custom_selinux_policy rackname=<RACKNAME> cabinetname=<CABINETNAME> component=<COMPONENT> [targetComponentName=<LISTOFCOMPONENTS> sendall=<True or False>]\n Send a request to EC to get a new policy based on the existing violations of the component, in case there is a new policy created by Exacloud, ECRA will automatically send it back to EC in order to apply it",
        "validatexml": "[rackname]\n Validates XML against database nodes, if no rackname is given, validation is done in whole ECRA fleet",
        "getxmlinfo": "getxmlinfo [rackname=]<rackname> | xmlpath=<xml_file_path>\nGet info from an XML given its rack name or an external file path",
        "xml patchnodes": "rackname=<rack name> inventories=<comma-separated inventories> [updaterack=true]\n Patch the rack XML with the  provided missing inventories.",
        "updatexml": "rackname=<rack name> xml=<original|updated> path=<xml_file_path> [force=<true|false>]. Updates the xml for the specified rack with the xml in the provided path.\nUse force=true to update the xml even if there are ongoing operations."
    },
    "exaservice": {
        "create": "racksize=<eighth|quarter|half|full> model=<X4-2|X5-2|X6-2|X7-2> [rack_cores=rc] [node_subset=<nsubset>] [clu_size=SMALL|MEDIUM|LARGE|WHOLE] [clu_cores=<c>] [clu_memory=<m>] [clu_storage=<s>] [clu_ohsize=<o>] [clu_name=<cn>] [inst_name=<in>] [enable_sparse=Y|N][backupto_disk=Y|N][type=subscription|metered][ipNet=Y|N] [admin_username=<username>] [appidUser=<appid>] [appidPwd=<paswd>] [client_nw=<c_nw>] [backup_nw=<b_nw] [entity_id=<ent_id>]\nCreates a multivm exaservice with one cluster. <nsubset> is a comma separated list of compute nodes (aliases: specify node-1 or 1 for the first node) on which this cluster should run. Specifying <clu_size> will set defaults for <clu_cores>, <clu_memory>, <clu_storage> and <clu_ohsize>. <clu_cores> is the total cores for the cluster, <clu_memory> the total memory in GB for the cluster, <storage> the total ASM storage in TB for the cluster and <clu_ohsize> the per compute node OH size in GB with a minimum of 50GB. <rack_cores> is the total cores purchased for the rack.<enable_sparse> and <backupto_disk> both default to N.<ent_id> is the TAS entity id or subscription id. rack_name=<name of the rack from rackslot> exadata_id=<exadata id",
        "addcluster": "exaservice_id=<id> [node_subset=<nsubset>][clu_size=SMALL|MEDIUM|LARGE|WHOLE] [clu_cores=<c>] [clu_memory=<m>] [clu_storage=<s>] [clu_ohsize=<o>] [clu_name=<cn>] [enable_sparse=Y|N][backupto_disk=Y|N] [ipNet=Y|N] [admin_username=<username>] [appidUser=<appid>] [appidPwd=<paswd>] [client_nw=<c_nw>] [backup_nw=<b_nw] [entity_id=<ent_id>]\nAdd a cluster to a multivm exaservice. <nsubset> is a comma separated list of compute nodes (aliases: specify node-1 or 1 for the first node) on which this cluster should run.  Specifying <clu_size> will set defaults for <clu_cores>, <clu_memory>, <clu_storage> and <clu_ohsize>.  <clu_cores> is the total cores for the cluster, <clu_memory> the total memory in GB for the cluster, <storage> the total ASM storage in TB for the cluster and <clu_ohsize> the per compute node OH size in GB with a minimum of 50GB. <enable_sparse> and <backupto_disk> both default to N.<ent_id> is the TAS entity id or subscription id. rack_name=<name of the rack from rackslot>",
        "reshape": "exaservice_id=<id> < rack_cores=<rc> | burst_cores=<bc> | [add_cell=<ac> | asmPower=<ap>] | add_compute=<aco> > [<rack_cores_dist=<rcd> | burst_cores_dist=<bcd>>]\n Updates exaservice resource params.  <rc> is the new value of the total number of subscribed cores for the exaservice, <bc> is the new value of the total number of burst cores for the exaservice. Only one of <rc> or <bc> can be specified. <rcd> (and <bcd>) is a comma separated list of cores for each node and the sum of the cores in the list should equal <rc> (or <bc> if <bcd> was specified). <ac> specifies the additional number of storage cells that are going to be attached to the current exaservice as per the elastic functionality. <ap> Is a number for the processing power that is going to be assigned to rebalance existing information to new added storage cells. <aco> Is the additional number of computes that are going to be attached to the current exaservice as per the elastic functionality",
        "reshapecluster": "exaservice_id=<id> exaunit_id=<eid> [node_subset=<nsubset>] < clu_cores=<c> | clu_memory=<m> | clu_storage=<s> | clu_ohsize=<p>>\nOne of cluster resources <c>, <m>, <s> or <p> can be modified using the reshape cluster operation. The nodes on which the cluster should run can be changed using <nsubset> which is a comma separated list of compute nodes (aliases: specify node-1 or 1 for the first node) on which this cluster should run.",
        "coreburstcluster": "exaservice_id=<id> exaunit_id=<eid> clu_cores=<c>\n Burst cluster <eid> to <c> cores. Please not this command is disbaled for now.",
        "deletecluster": "exaservice_id=<id> exaunit_id=<eid>\nDeletes a cluster (exaunit) associated with a exaservice.",
        "delete": "exaservice_id=<id> [sse=Y|N]\n Deletes the multi-vm exaservice. Setting sse(skip secure erase) to N will erase all ASM storage including any metadata and shred VM images on compute nodes",
        "list": "[exaservice_id=<id>] [exaunit_id=<eid>].\n Returns the clusters under the exaservice <id> including the resources allocated. If <id> is not specified, lists the created exaservice Ids without the cluster information",
        "migrate": "json=<jfile> [outfile=ofile] [mode=mvm-to-nodesubset]\n Migrates a single vm instance to multi-vm instance with one cluster. jfile is the file that contains the migration input json. Specify <mode> if migrating from multi-vm phase 1 to a subsequent phase. For details of the content of the json file, please see User_manual.txt.  ",
        "migratestatus": "service_id=<sid> [outfile=<ofile>][mode=mvm-to-nodesubset]\nReports the status of migration for a given single-vm instance <sid>. If <ofile> is specified, will append a detailed status to the file. Specify <mode> to fetch status of migration from multi-vm phase 1 to a subsequent phase",
        "migrateprefetch": "<[exaunit_id=<eid>] | [service_id=<sid>]> [outfile=<ofile>]\nPrefetches the input json template needed for the migrate subcommand given one of <eid> or <sid>. If <outfile> is specified, the template json is appended to it.",
        "migratepostfetch": "exaservice_id=<id> entity=<ent> [exaunit_id=<eid>] [outfile=<ofile>]\n Post migration to multi-vm, fethches the requested entity <ent> information for a multi-vm service <id>",
        "migratepostupdate": "exaservice_id=<id> input_json=<injson>\nPost migration to multi-vm, updates an entity using the payload in <injson> for a multi-vm service <id>",
        "migraterollback": "<service_id=<sid> [mode=<mvm-to-nodesubset>]\n Rolls back a succesfully migrated single-vm instance <sid> back to single-vm. Specify <mode> to fetch status of migration from multi-vm phase 1 to a subsequent phase.",
        "suspend": "exaservice_id=<id> \n Shuts down all clusters associated with the exaservice.",
        "resume": "exaservice_id=<id> \n Resumes ( Start up)  all clusters   associated with the exaservice.",
        "rackinfo": "exaservice_id=<id> \n Fetch all rackinfo and rack status corresponding to the exaservice id",
        "syncrackslots": "exadata_id=<exadata_id> \n Sync rack_slots missing in racks table with current elastic inventory"
    },
    "zone": {
        "add": "<zone_name> uri=<zonal ecra uri> username=<username> passwd=<passwd> [region=<region> | dc=<data center> | location=<LOCAL|REMOTE> | bkupuri=<zonal ecra backup uri>] \n Add a new zone either remote or local with the URI of the ECRA service. Location will default to REMOTE if not specified.",
        "list": "[ racksize=<racksize> [hwmodel=<hwmodel>] [dc=<data-centre>] [region=<region>] ]\nLists all zonal ECRAs. If params are specified, param racksize is mandatory",
        "delete": "<zone_name> \n Delete the given zone from the local ECRA."
    },
    "location": {
        "add": "<location_name> uri=<zonal ecra uri> username=<username> passwd=<passwd> type=<location type AD/Zone/Site> [region=<region> | dc=<data center> | subscription_id=<subscription id of site> | compute_zone=<compute_zone for AD>",
        "list": "type=<location type> [ [dc=<data-centre>] [region=<region>] [subscription_id=<subscription id of the site>] [compute_zone=<compute_zone>]   ]\nLists all locations of ECRAs. ",
        "delete": "<location_name> type=<location type> [dc=<data-centre>] [region=<region>] [subscription_id=<subscription id of the site>] [compute_zone=<compute_zone>]]\nDelete location of ECRAs. ",
        "details": "subscription_id=<subscription id> racksize=<racksize | exadata size> model=<model> region=<region>\n Get the details like features, capacity of location(s) fetched based on the command line arguments."
    },
    "idemtoken": {
        "new": "Generate a new idemtoken.",
        "renew": "<idemtoken=idemtoken> Renew the idemtoken to allow it to be used again"
    },
    "exadata": {
        "network_info": "[racksize=<racksize> [zone=<zone name>]] \n Find the network information for the given racksize and zonename",
        "models": "[model=<modelname>] \nGet All models info about ilom type and support status."
    },
    "capacity": {
        "register": "exadata_id=<exadata id>  tmpl_xml=<template xml path> \nRegister exadata with given exadata id.",
        "deregister": "exadata_id=<exadata id> [delete_inventory=<Y/N>]\nDeregister exadata associated with given exadata id, it can delete also all the inventory info while deregistering.",
        "list": "[racksize=<racksize> | exadata_size=<exadata size> | model=<model> | location=<location> | status=<status> | exadata_id=<exadata_id> | atp=<Y|N>] \nList numbers of racks on the given racksize, model, location and status.",
        "get": "[exadata_size=<exadata size> | model=<exadata model> | status=<status> | exadata_id=<exadata id>] \nGet details of the exadata and associated racks satisfying given parameters.",
        "reserve": "idemtoken=<idemtoken> [racksize=<racksize> | exadata_size=<exadata size> | exadata_id=<exadata_id> | model=<model> | fabricname=<name> | location=<location> | atp=<Y/N> | provisionType=<preprovision/reconfig> |  clustertag=<tag> | dbSystemId=<dbSystemId> dom0_bonding=<true/false>] [mvmbonding=true/false] [sitegroup=<sitegroupvalue>] \nReserve an available rack on the given racksize, model and location. idemtoken is required for each request. Specify exadata_id if a specific exadata has to be reserved.",
        "release": "exadata_id=<exadata id> \nRelease exadata associated with given exadata id.",
        "move": "racksize=<racksize> destinationPool=<atp|exacs> [model=<model>][numberOfRacks=<number>][rackname=name][clustertag=tagname] \n Moves the exadata racks between ExaCS and ATP pools, or it can be run for a specific rack.",
        "getavailability": "[type=<rack|elastic|all>] [detailed=<true|false>] [raw=<true|false>]\n Gives a report that tells how many racks or elastic nodes are free, used or in other states and the total existent.With the datailed option enabled, it separates the racks/nodes by model and raw option will show you the output in json format",
        "getfilesystemdefinitions": "model=<RACK MODEL> [exa0cid=<EXAOCID> rackname=<rackname> adbd=<yes|no> svm=<yes|no>]\n Returns the minimum and maximum size for each mount of the filesystem for the indicated model and the extra parameters.",
        "compatibility": {
            "addmatrix": "osversion=<i.e. ol8,ol7 split by comma> exaversion=<i.e. 23.x.x up to 2 level and split by comma> hwmodel=<i.e X10M-2 split by comma> giversion=<i.e. 23c,21c split by comma> servicetype=<i.e. EXACS only one value allowed>",
            "addexclusion": "osversion=<i.e. ol8,ol7 split by comma> exaversion=<i.e. 23.x.x up to 2 level and split by comma> hwmodel=<i.e X10M-2 split by comma> giversion=<i.e. 23c,21c split by comma> servicetype=<i.e. EXACS only one value allowed>",
            "updatematrix": "id=<id number> [ osversion=<i.e. ol8,ol7 split by comma> | exaversion=<i.e. 23.x.x up to 2 level and split by comma> | hwmodel=<i.e X10M-2 split by comma> | giversion=<i.e. 23c,21c split by comma> | servicetype=<i.e. EXACS only one value allowed> | status=<ENABLED|DISABLED|EXCLUDED> ]",
            "enablematrix": "id=<id number>",
            "disablematrix": "id=<id number>",
            "list": "[id=<id number> | hwmodel=<i.e X10M-2 split by comma> | giversion=<i.e. 23c,21c split by comma> | servicetype=<i.e. EXACS only one value allowed> | exaversion=<i.e. 23.x.x up to 2 level and split by comma>]",
            "catalog": "[model=<i.e X10M-2> | giversion=<i.e. 23c,21c> | servicetype=<i.e. EXACS> | dom0version=<xx.x.x>] Returns the catalog based on the arguments"
        },
        "gioperation": "jsonpath=<JSONPATH> \n Send a request to EC to add/update/delete a GI, the operation comes inside the payload in the type attribute."
    },
    "rackslot": {
        "list": "exadata_id=<exadata id> \nGet rack slots list for given exadata id.",
        "get": "rackname=<rack name> \nGet rack slot details for given rack name.",
        "register": "exadata_id=<exadata id> [networkinfo=<network info json path>|xml=<rack xml path>] \nRegister rack slot with given exadata id.",
        "deregister": "rackname=<rack name> \nDeregister rack slot associated with given rack name."
    },
    "sshkey": {
        "add": "<exaunit_id> <sshkey=<path-to-shh-key>> [users=oracle/opc/root][vms=_all_/specific vm][sshcomment=<ssh comment>]\nAdd sshkey on the given exaunit for specified vms.",
        "delete": "<exaunit_id>[vms=_all_/specific vm] \nDelete sshkey on the given exaunit for specified vms.",
        "rescue": "<exaunit_id> [vms=_all_/specific vm]\nRescue sshkey on the given exaunit for specified vms.",
        "get": "<exaunit_id=exaunit_id> [host=<hostname of machine/all>] [user=<username for which you need key root/oracle>] [node_type=<dom0/domu/ibswitch/cell/all_nodes>] \nGet the SSH key information will work only for ops user.",
        "createdomukey": "<exaunit id> [domu=<domu>] [user=<user>] \n Crea a new temporal SSH Key pair to get access to DomU",
        "verifydomukey": "<exaunit id> domu=<domu> user=<user> \n  Verify if the SSH key pair associated to the specified VM and user is still operational.",
        "deletedomukey": "<exaunit id> [domu=<domu>] [user=<user>] \n Delete the private SSH key associated to the specified exaunit, domU and user.",
        "getdomukey": "id=<request id> \n Retrieve the public SSH key generated for the provided request.",
        "addadbskey": "<exaunit id> [idemtoken=<idemtoken>] \n Add the adbs key to KMS",
        "removeadbskey": "<exaunit id> [idemtoken=<idemtoken>] \n Remove the adbs key from KMS"
    },
    "iorm": {
        "get_dblist": "<exaunit_id> \nGet the list of DB instances on the cells.",
        "get_flashsize": "<exaunit_id> \nGet the flash cache size from the cells.",
        "get_obj": "<exaunit_id> [idemtoken=<idemtoken>]  \nGet the IORM objective from the cells.",
        "set_obj": "<exaunit_id> \nSet the IORM objective on the cluster cells.",
        "set_obj_v2": "<exaunit_id> \nSet the IORM objective on the cluster cells.",
        "set_dbplan": "<exaunit_id> \nSet an IORM DB Plan on the cells.",
        "set_dbplan_v2": "<exaunit_id> \nSet an IORM DB Plan on the cells, worflow based",
        "get_dbplan": "<exaunit_id>  [idemtoken=<idemtoken>] \nGet the IORM DB Plan from the cells.",
        "reset_dbplan": "<exaunit_id> \nRemove the current IORM DB Plan from the cells, and reset it to None.",
        "reset_dbplan_v2": "<exaunit_id> \nRemove the current IORM DB Plan from the cells, and reset it to None.",
        "get_clientkeys": "<exaunit_id> \n Get the list of client keys from the cells.",
        "get_pmemsize": "<exaunit_id> \nGet the pmem cache size from the cells.",
        "get_resources": "<exaunit_id> \nGet the iorm total cache resources from the cells in G.",
        "set_clusterplan": "<exadatainfrastructureId> \nSet an IORM Cluster Plan on the cells",
        "get_clusterplan": "<exadatainfrastructureId> \nget an IORM Cluster Plan on the cells"
    },
    "dg": {
        "create": "<exaunit_id>\n Creates a DB with DG on the given exaunit. This must only be run on the primary exaunit. Please fill out /tmpl/dg_setup.json.",
        "config": "<exaunit_id> <instance=primary/standby>\n Configure connectivity on the provided instance, eg: if primary is provided ecracli picks up the json from tmpl/dg_config_primary.json which should hold the standby cluster's info and vice versa. Please fill out the necessary json under tmpl/",
        "setup": "<exaunit_id> \n Sets up DB software on the given exaunit. This must only be run on the standby exaunit. Please fill out /tmpl/dg_setup.json."
    },
    "ebtables": {
        "enable": "<exaunit_id>\n Activate ebtables on the chosen cluster",
        "disable": "<exaunit_id>\n Deactivate ebtables on the chosen cluster",
        "add": "<exaunit_id> IP=<ip1,ip2,...>\n Add ebtables rules on the chosen cluster",
        "delete": "<exaunit_id> IP=<ip1,ip2,...>\n Del ebtables rules on the chosen cluster"
    },
    "jumbo": {
        "enable": "<exaunit_id>\n Activate jumbo for the chosen cluster",
        "disable": "<exaunit_id>\n Deactivate jumbo for the chosen cluster",
        "query": "<exaunit_id>\n Query Jumbo for the chosen cluster"
    },
    "info": "[display=cei] [hide_clusters=<True|False>] [shownongoingops=<True>|False] [rackstate=<value>] [rackname=<rackname_substring>] [verbose=yes|no] [ecraonly=yes|no] Displays a list of current settings and connect to endpoint to pull the ecra version information. If [hide_clusters] parameter is provided then the list of clusters is not retrieved or shown. If parameter showongoingops is used, ongoing operations in cluster/cabinets non provisioned is shown. Parameters rackname and rackstate can be used to filter output from command.\nUse verbose option to display all component versions.\nUse ecraonly option to retrieve information only from ecra, exacloud will not be called if ecraonly=yes",
    "create_sdb": "Issue a create service request and wait for completion. Upon completion, create a starter database on this exaunit",
    "test": {
        "createdelete": "Issue a create service request and wait for completion. Upon completion, delete it"
    },
    "update_cores": "<exaunit_id> [subscriptionOcpu/meteredOcpu/burstOcpu=<num>] [json_path]\nUpdate cores allocation info for the given exaunit, json path contains json with cores for each vm",
    "switchmode": "mode=<brokerproxy|default> [cimid=<cimid>]\nSwitch ecracli to given mode. For 'brokerproxy' mode, cimid is a a required parameter and should refer to a valid service instance. By default, ecracli will start in 'default' mode.",
    "health_check": [
        "If no option is mentioned, then it runs health check on all clusters in all the exadata machines (single-VM and multi-VM) ",
        "<rack_name>\nDo a cluster health check for the given rack. If rack name is not given, this command will check every cluster registered in this ecra instance.",
        "<rack_name> target_hosts=dom0s,domUs,cells,switches \n It runs health check on the hosts (mentioned in target_hosts) associated with the <rackname>.",
        "<exadata_id> \n It runs health check on the exadata machine (exadata_id) by first running health check on dom0s, cells and switches and then on all the domUs in that exadata machine.",
        "If envcheck is appended after <rack_name> then it will run ltwt connectivity check for the given rack, and by default it will skip domu nodes, append domucheck=True to include domu nodes.",
        "If exachk is appended after <rack_name> then it will run exachk utility with input json provided at /tmpl/exachk.json. Please modify /tmpl/exachk.json to customize,",
        "If profilepath is appended after <rack_name> then customized checks (defined in custom_profile.json) can be run",
        "append arg which supports all command line options in same format used with exachk cli.",
        "*) Examples:",
        "health_check",
        "health_check <exadata_id>",
        "health_check <rack_name>",
        "health_check <rack_name> [target_hosts=dom0s,domUs,cells,switches]",
        "health_check <rack_name>/envcheck [domucheck=<True|False>]",
        "health_check <rack_name>/exachk [arg=<arg>]",
        "health_check <rack_name> profilepath=</file_path/custom_profile.json>"
    ],
    "clone": {
        "testmaster": "<exaunit_id> [sourceDB=<src db> | tmDB=<testmaster DB> | passwd=<password>]\nCreate a testmaster on the cluster",
        "snap": "<exaunit_id> [cloneDB=<clone db> | tmDB=<testmaster DB> | passwd=<password>]\n Create a snapclone on the cluster"
    },
    "patch": [
        "<target> <cluster_name(s)> [op=<operations>] [serviceType=<serviceType>] [exasplice=yes|no]  [rackModel=X9M-2]",
        "<target> <exaocid=<exaocid>> [op=<operations>] [serviceType=<serviceType>] [TargetVersion=<targetVersion>] in case of cps patching. For cps patching details, type help patch cps",
        "Apply Exadata patch operation for given target type (cell, dom0, domu, ibswitch, qfab) in each given cluster.",
        "*) Examples of dom0 , domu , qfab , cell and ibswitch patching:",
        "    patch dom0 slcs08adm07adm08 serviceType=EXACS",
        "    patch dom0 slcs08adm07adm08 serviceType=EXACS rackModel=X9M-2",
        "    patch domu slcs16adm0708clu12 serviceType=EXACS",
        "    patch dom0+cell slcs08adm07adm08 serviceType=EXACS",
        "    patch dom0+cell slcs08adm07adm08 scas07adm0304 serviceType=EXACS",
        "    patch dom0 slcs08adm07adm08 op=patch serviceType=EXACS",
        "    patch dom0 slcs08adm07adm08 op=patch serviceType=EXACS exasplice=yes",
        "    patch domu slcs16adm0708clu12 op=patch serviceType=EXACS",
        "    patch domu slcs16adm0708clu12 op=patch serviceType=EXACS SkipGiDbValidation=yes",
        "    patch cell slcs08adm07adm08 op=patch serviceType=EXACS exasplice=yes",
        "    patch dom0+cell slcs08adm07adm08 op=patch serviceType=EXACS",
        "    patch dom0 slcs08adm07adm08 op=rollback exasplice=yes serviceType=EXACS",
        "    patch dom0 slcs08adm07adm08 op=patch_prereq_check exasplice=no serviceType=EXACS SkipDomuCheck=yes",
        "    patch dom0 slcs08adm07adm08 op=patch_prereq_check EnablePlugins=yes PluginTypes=dom0domu serviceType=EXACS",
        "    patch dom0 slcs08adm07adm08 op=patch exasplice=no serviceType=EXACS SkipDomuCheck=yes",
        "    patch dom0 slcs08adm07adm08 op=rollback exasplice=no serviceType=EXACS SkipDomuCheck=yes",
        "    patch domu all op=patch_prereq_check serviceType=EXACS",
        "    patch domu slcs08adm07adm08 op=patch_prereq_check EnablePlugins=yes PluginTypes=domu serviceType=EXACS",
        "    patch domu all op=postcheck serviceType=EXACS",
        "    patch dom0 slcs08adm07adm08 op=patch BackupMode=no serviceType=EXACC",
        "    patch dom0 slcs08adm07adm08 op=patch EnablePlugins=yes PluginTypes=dom0+dom0domu serviceType=EXACC",
        "    patch dom0 slcs08adm07adm08 op=patch EnablePlugins=yes PluginTypes=dom0 serviceType=EXACC",
        "    patch domu slcs08adm07adm08 op=patch EnablePlugins=yes PluginTypes=domu serviceType=EXACC",
        "    patch domu slcs08adm07adm08 op=patch serviceType=EXACC EnableDBHealthChecks=yes",
        "    patch dom0+cell scas07adm0304clu1 op=patch_prereq_check force=yes serviceType=EXACS",
        "    patch dom0 slcs16adm0304clu4 op=patch serviceType=FA exasplice=yes",
        "    patch dom0 slcs16adm0304clu4 op=rollback serviceType=FA exasplice=yes",
        "    patch dom0 slcs16adm0304clu4 op=patch_prereq_check serviceType=FA",
        "    patch dom0 slcs16adm0304clu4 op=patch_prereq_check serviceType=FA exasplice=yes",
        "    patch domu slcs16adm0304clu4 op=patch_prereq_check serviceType=FA exaunit=xyz",
        "    patch dom0 slcs16adm0304clu4 op=patch_prereq_check serviceType=FA exaocid=abc",
        "    patch dom0 slcs16adm0304clu4 op=patch_prereq_check serviceType=FA exaocid=abc SkipDomuCheck=yes",
        "    patch dom0 slcs16adm0304clu4 op=patch serviceType=FA exaocid=abc SkipDomuCheck=yes",
        "    patch dom0 slcs16adm0304clu4 op=rollback serviceType=FA exaocid=abc SkipDomuCheck=yes",
        "    patch dom0 slcs08adm07adm08 op=oneoff",
        "    patch domu slcs08adm07adm08 op=oneoff",
        "    patch cell slcs08adm07adm08 op=oneoff",
        "    patch switch slcs08adm07adm08 op=oneoff",
        "    patch switch slcs08adm07adm08 op=oneoff plugin_location=<path of the oneoff files>",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACS",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACS",
        "    patch switch slcs08adm07adm08 op=rollback serviceType=EXACS",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchSwitchType=admin",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC PatchSwitchType=admin",
        "    patch switch slcs08adm07adm08 op=rollback serviceType=EXACC PatchSwitchType=admin",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchSwitchType=all",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC PatchSwitchType=all",
        "    patch switch slcs08adm07adm08 op=rollback serviceType=EXACC PatchSwitchType=all",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchSwitchType=roce",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC PatchSwitchType=roce",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchSwitchType=rocespine",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC PatchSwitchType=rocespine",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchSwitchType=spine",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC PatchSwitchType=spine",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchSwitchType=none",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC PatchSwitchType=none",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchSwitchType=null",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC PatchSwitchType=null",
        "    patch switch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC",
        "    patch switch slcs08adm07adm08 op=patch serviceType=EXACC",
        "    patch switch slcs08adm07adm08 op=oneoff",
        "    patch switch slcs08adm07adm08 op=oneoff plugin_location=<path of the oneoff files>",
        "    patch ibswitch slcs08adm07adm08 op=oneoff",
        "    patch ibswitch slcs08adm07adm08 op=oneoff plugin_location=<path of the oneoff files>",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACS",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACS",
        "    patch ibswitch slcs08adm07adm08 op=rollback serviceType=EXACS",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchibswitchType=admin",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC PatchibswitchType=admin",
        "    patch ibswitch slcs08adm07adm08 op=rollback serviceType=EXACC PatchibswitchType=admin",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchibswitchType=all",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC PatchibswitchType=all",
        "    patch ibswitch slcs08adm07adm08 op=rollback serviceType=EXACC PatchibswitchType=all",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchibswitchType=roce",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC PatchibswitchType=roce",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchibswitchType=rocespine",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC PatchibswitchType=rocespine",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchibswitchType=spine",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC PatchibswitchType=spine",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchibswitchType=none",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC PatchibswitchType=none",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC PatchibswitchType=null",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC PatchibswitchType=null",
        "    patch ibswitch slcs08adm07adm08 op=patch_prereq_check serviceType=EXACC",
        "    patch ibswitch slcs08adm07adm08 op=patch serviceType=EXACC",
        "    patch ibswitch slcs08adm07adm08 op=oneoff",
        "    patch ibswitch slcs08adm07adm08 op=oneoff plugin_location=<path of the oneoff files>",
        "    patch dom0 slcs08adm07adm08 op=oneoffv2 OneoffScriptAlias='Suricata'",
        "    patch cell slcs08adm07adm08 op=oneoffv2 OneoffScriptAlias='ParamUpdate'",
        "    patch dom0 slcs08adm07adm08 op=oneoffv2 IncludeNodeList=slcs27adm03.us.oracle.com OneoffScriptAlias='Iptables'",
        "    patch cell slcs08adm07adm08 op=oneoffv2 IncludeNodeList=slcs27celadm03.us.oracle.com OneoffScriptAlias='ParamUpdate'",
        "    patch dom0 SEA200420 op=patch_prereq_check ClusterLess=yes",
        "    patch dom0 SEA200420 op=patch ClusterLess=yes",
        "    patch dom0 SEA200420 op=rollback ClusterLess=yes",
        "    patch cell SEA200420 op=patch_prereq_check ClusterLess=yes LaunchNode=<dom0Node>",
        "    patch cell SEA200420 op=patch ClusterLess=yes LaunchNode=<dom0Node>",
        "    patch dom0 SEA200420 op=patch ClusterLess=yes LaunchNode=<dom0Node> LaunchNodeType=COMPUTE",
        "    patch dom0 SEA200420 op=patch ClusterLess=yes LaunchNode=<management_host> LaunchNodeType=MANAGEMENT_HOST",
        "    patch cell SEA200420 op=patch ClusterLess=yes LaunchNode=<dom0Node> LaunchNodeType=COMPUTE",
        "    patch cell SEA200420 op=patch ClusterLess=yes LaunchNode=<management_host> LaunchNodeType=MANAGEMENT_HOST",
        "    patch cell SEA200420 op=rollback_prereq_check ClusterLess=yes LaunchNode=<dom0Node>",
        "    patch cell SEA200420 op=rollback ClusterLess=yes LaunchNode=<dom0Node>",
        " -> Include node list patch options. For Dom0 and cells, FQDN details can be obtained from the cluster xml file. For DomU, client hostname need to be considered when input arguments are passed.",
        "    patch dom0 slcs27adm0304clu3 op=patch_prereq_check serviceType=EXACC IncludeNodeList=slcs27adm03.us.oracle.com",
        "    patch dom0 slcs27adm0304clu3 op=patch serviceType=EXACC IncludeNodeList=slcs27adm03.us.oracle.com",
        "    patch dom0 slcs27adm0304clu3 op=rollback serviceType=EXACC IncludeNodeList=slcs27adm03.us.oracle.com",
        "    patch dom0 slcs27adm0304clu3 op=patch_prereq_check serviceType=EXACC IncludeNodeList=slcs27adm03.us.oracle.com,slcs27adm04.us.oracle.com",
        "    patch dom0 slcs27adm0304clu3 op=rollback serviceType=EXACC IncludeNodeList=slcs27adm03.us.oracle.com,slcs27adm04.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=patch_prereq_check serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=patch serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=rollback serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=rollback_prereq_check serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=postcheck serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=postcheck serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com,slcs27admcel04.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=patch_prereq_check serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com,slcs27admcel04.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=patch serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com,slcs27admcel04.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=rollback serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com,slcs27admcel04.us.oracle.com",
        "    patch cell slcs27adm0304clu3 op=rollback_prereq_check serviceType=EXACC IncludeNodeList=slcs27admcel03.us.oracle.com,slcs27admcel04.us.oracle.com",
        "    patch domu slcs27adm0304clu3 op=patch_prereq_check serviceType=EXACC IncludeNodeList=slcs27admdv0405m.us.oracle.com",
        "    patch domu slcs27adm0304clu3 op=patch serviceType=EXACC IncludeNodeList=slcs27admdv0405m.us.oracle.com",
        "    patch domu slcs27adm0304clu3 op=rollback serviceType=EXACC IncludeNodeList=slcs27admdv0405m.us.oracle.com",
        "    patch domu slcs27adm0304clu3 op=patch_prereq_check serviceType=EXACC IncludeNodeList=slcs27admdv0405m.us.oracle.com,slcs27admdv0305m.us.oracle.com",
        "    patch domu slcs27adm0304clu3 op=rollback serviceType=EXACC IncludeNodeList=slcs27admdv0405m.us.oracle.com,slcs27admdv0305m.us.oracle.com",
        "    patch domu slcs27adm0304clu3 op=postcheck serviceType=EXACC IncludeNodeList=slcs27admdv0405m.us.oracle.com,slcs27admdv0305m.us.oracle.com",
        "*) Examples of cps patching (For cps patching details only, type help patch cps):",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=patch serviceType=EXACC TargetVersion=20.1.2.0.0.200930",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=patch_prereq_check serviceType=EXACC TargetVersion=LATEST",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=image_backup serviceType=EXACC",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=patch serviceType=EXACC ForceRemoveCustomRpms=yes",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=patch serviceType=EXACC AllowActiveNfsMounts=no",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=patch serviceType=EXACC IgnoreAlerts=no",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=patch serviceType=EXACC ModifyAtPrereq=no",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=rollback serviceType=EXACC ModifyAtPrereq=no",
        "    patch cps exaocid=ocid1.exadatainfrastructure.oc1.eu-frankfurt-1 op=rollback serviceType=EXACC TargetVersion=19.2.4.0.0.190709 ModifyAtPrereq=no",
        "*)  Example of qfab patching",
        "    patch qfab sea2xx2xx0061 op=patch TargetVersion=22.1.5.0.0.221111.1",
        "    patch qfab sea2xx2xx0061 op=rollback",
        "    patch qfab sea2xx2xx0061 op=patch_prereq_check",
        "*) valid patching targets:",
        "    -> dom0,domu,cell,ibswitch,cps,qfab",
        "*) valid cluster name(s):",
        "    -> all - will run the specified patch operation on all the provisioned clusters in the ECRA",
        "    -> cluster_name(s) -> comma sperarated list of cluster names which need to be patched",
        "    -> [exaocid=<exaocid>] -> in case of a cps patching",
        "*) operations available with op parameter",
        "    -> patch                 [default]",
        "    -> rollback              [rollback the patches ]",
        "    -> patch_prereq_check    [precheck for patching]",
        "    -> postcheck             [postcheck for patching]",
        "    -> rollback_prereq_check [precheck for rollback]",
        "    -> backup_image          [image backup on dom0 and domU]",
        "    -> oneoff                [To apply one off related fixes]",
        "    -> oneoffv2              [To apply one off v2 implementation related fixes]",
        "*) valid BackupMode targets:",
        "    -> dom0,domu",
        "*) valid EnablePlugins options:",
        "    -> yes                   [enable to run plugins]",
        "    -> no                    [disable to run plugins]",
        "*) valid PluginTypes options:",
        "    -> domu                  [Run plugins on domu node only]",
        "    -> dom0+dom0domu         [Run plugins on both dom0's domu node and dom0 node]",
        "    -> dom0                  [Run plugins on dom0 node only]",
        "    -> dom0domu              [Run plugins on dom0's domu node only]",
        "*) valid force options:",
        "    -> yes                   [To disable interactive prompt for non-default additional options specified in exadata_infra_patch_extra_params.json]",
        "    -> no                    [Default option.Prompts user confirmation for non-default additional options specified in exadata_infra_patch_extra_params.json]",
        "*) valid exasplice options:",
        "    -> yes                   [Indicate Monthly/ExaSplice Patching]",
        "    -> no                    [Disable monthly patching. Default is no]",
        "*) valid serviceType options:",
        "    -> EXACS                 [Exadata cloud service]",
        "    -> FA                    [Fussion Aps]",
        "    -> ADBS                  [Autonomous DB Server]",
        "    -> ADBD                  [Autonomous DB Dedicated]",
        "    -> EXACC                 [ExaCloud at Customer]",
        "    -> PREPROD               [Pre Production]",
        "    -> TEST                  [Test Systems]",
        "    -> GBU                   [Global Business Unit]",
        "    -> IDCS                  [Oracle Identity Cloud Service]",
        "    -> OMCS                  [Oracle Managed Cloud Service]",
        "*) valid rackModel values:",
        "    -> X6-2                  [Exadata Rack Model]",
        "    -> X7-2                  [Exadata Rack Model]",
        "    -> X8-2                  [Exadata Rack Model]",
        "    -> X8M-2                 [Exadata Rack Model]",
        "    -> X9M-2                 [Exadata Rack Model]",
        "    -> X10M-2                [Exadata Rack Model]",
        "    -> X10M-2L               [Exadata Rack Model]",
        "    -> X10M-2XL              [Exadata Rack Model]",
        "*) exaunit:",
        "    -> You can specify valid exaunit id when patching domUs",
        "*) exaocid:",
        "    -> You can specify valid exaocid when patching Dom0, Cell and Switch",
        "*) Usage notes:",
        "    o The ibswitch and domu are only supported as standalone targets",
        "    o We can perform few special patching operations which needs to be used very cautiously. Same can",
        "      be passed using additional options to patchmgr command and for upgrade/rollback the specified ",
        "      compute nodes in a given cluster. We can specify same through additional options using",
        "      exadata_infra_patch_extra_params.json or call patching rest API with these options.\n",
        "*) Examples of patch debugging information",
        "      patch getDebugInfo workflowId=<workflowId>",
        "      *) Examples of patch getDebugInfo CLI with workflowId specified:",
        "      patch getDebugInfo workflowId=b41870a6-4d02-4a0d-9523-00d95ffd0567",
        "      *) Mandatory Parameters:",
        "      -> workflowId [where workflowId is the id of the workflow]\n",
        "       patch getDebugInfo ecraRequestId=<Request Id in ECRA DB>",
        "       *) Examples of patch getDebugInfo CLI with ECRA Request ID specified:",
        "       patch getDebugInfo ecraRequestId=fe4ca038-557d-498b-b440-eb4409d1d2f9",
        "       *) Mandatory Parameters:",
        "       -> ecraRequestId [where ecraRequestId is the id of the Request in ECRA DB]\n",
        "     patch getDebugInfo rackName=<rackName> days=<debug info for operations on this rack for the last n days>",
        "     *) Examples of patch getDebugInfo CLI with rackName specified:",
        "     patch getDebugInfo rackName=slcs27adm0102clu7 ",
        "     patch getDebugInfo rackName=slcs27adm0102clu7 days=5",
        "    *) Mandatory Parameters:",
        "    -> rackName [rackName on which you want to debug]",
        "    *) Optional Parameters:",
        "    -> days [debug info for all operations on this rack for the last n days, If not specified, default value is 7]"
    ],
    "status": [
        "<request id or status uuid or exaunit_id>",
        "Get the status information from a status uuid or a request id. If an exaunit id is entered, it will try to fetch the status of ongoing operation on that exaunit"
    ],
    "query_requests": [
        "<options> [limit=<num of records>] [raw=<true/false>]\nQuery requests using input options as query params",
        "Special options:",
        "1) limit",
        "You can set the limit of number of output request records by setting this option.",
        "By default there is no limit (limit=0)",
        "2) raw",
        "By default raw is set to False and ecracli will print out a nicely formatted output.",
        "By setting raw to either True or true, ecracli will print output in raw json format.",
        "3) range query",
        "To query range, add -lt or -gt at the end of the field.",
        "-> Examples:",
        "*) query_requests status=500",
        "*) query_requests status=202 operation=create-service",
        "*) query_requests status=202 start_time-gt=2016-06-24T16:50 start_time-lt=2016-06-25",
        "*) query_requests status=202 start_time-lt=2016-06-25 limit=0",
        "*) query_requests status=202 exaunit_id=100 limit=5 raw=true"
    ],
    "properties": {
        "get": "[<property_name>] \n Get the value of the property if name is specified, else get all properties.",
        "put": "<property_name> [value=<new value>] [description=<new description>]\n Update the property value and description",
        "gettypes": "Get all the property types that are in the env."
    },
    "cns": {
        "setup": "\nRun CNS setup which uploads zip files and topics to CNS",
        "run": "\nRun CNS collection of events and post to CNS",
        "post": "<event file>\nPOST the event payload  to CNS",
        "setupinterval": "\nSet job interval for infrastructure events collection",
        "enable": "\nenable CNS event collection job and post to CNS at ECRA level",
        "disable": "\ndisable CNS event collection and post to CNS at ECRA level",
        "enablerack": "\nenable CNS event collection and post for a given rack",
        "disablerack": "\ndisable CNS event collection and post for a given rack",
        "getrackstatus": "\nget enable/disable status of CNS for a given rack",
        "receive": "\n receive raw infra or ECRA events and post them to CNS"
    },
    "vmbackup": {
        "set_param": "rackname=<rackname> [remotebackup=enabled|disabled] [ossbackup=enabled|disabled] [maxremotebackups=<maxremotebackups>] [deletelocalbackup=<True|False>] [activeblockcommit=True|False] [materializedlocalcopy=True|False] [ossnumbackups=<ossnumbackups>]\n Sets the paramter (to a value) in vmbackup configuration file for the given rackname.\nNote: this allows usage of the same parameters but in snake_case: [remote_backup=enabled|disabled] [oss_backup=enabled|disabled] [max_remote_backups=<maxremotebackups>] [delete_local_backup=True|False] [active_block_commit=True|False] [materialized_local_copy=True|False] [oss_num_backups=<ossnumbackups>]",
        "get_param": "<rackname> <parameter>\n Gets the paramter value from vmbackup configuration file for the given rackname.\nNote: this method allows the same params in set_param, and also allows snake_case for the same params.",
        "install": "rackname=<rack_name> [skipchecksum=True|False] [deletelocalbackup=True|False] [remotebackup=enabled|disabled] [ossbackup=enabled|disabled <stauth=<auth uri>> <stuser=<user>> <stkey=<password>>] [skipimg=True|False] [materializedlocalcopy=True|False] [activeblockcommit=True|False] [clusterless=yes|no]\nCopy the vmbackup bits to dom0's. Use clusterless=yes for elastic cabinets and pass the cabinet name instead of rackname.\nNote: this allows usage of the same parameters but in snake_case: [skip_checksum=True|False] [delete_local_backup=True|False] [remote_backup=enabled|disabled] [oss_backup=enabled|disabled <stauth=<auth uri>> <stuser=<user>> <stkey=<password>>] [skip_img=True|False] [materialized_local_copy=True|False] [active_block_commit=True|False] [ClusterLess=yes|no]\nUse rackname=FLEET to install on every provisioned cluster.",
        "enable": "Enables vmbackup on the dom0s belonging to the given rackname",
        "disable": "Disables vmbackup on the dom0s belonging to the given rackname",
        "backup": "[exaunitid=<exaunit_id>] \nCreate a vm backup of all VM's.",
        "scheduler": {
            "setnextrun": "id=<job_id> datetime=<YYYY-DD-MM HH:mm:ss>\nGiven the job_id that is used for VMBackup (VMBackupJob, VMBackupStatusTrackerJob) will update the values to execute the next run in the given datetime (UTC)",
            "setfrequency": "id=<job_id> frequencyvalue=<1> frequency=<day|week|hour>\nGiven the job_id that is used for VMBackup (VMBackupJob, VMBackupStatusTrackerJob) will update the values to execute the job on the given frenquency"
        },
        "schedulerstatus": "[exaunitid=<exaunit_id> detailed=true] \nGet the status of the internal vmbackup scheduler job and the state of the nodes",
        "schedulerhistory": "exaunitid=<exaunit_id> [page=<page> pagesize=<pagesize] \nGet History of the given exaunitid, records are in descending order by when they were created",
        "rollback": "exaunitid=<exaunit_id> vmname=<vm_name> [backupseq=<>] [local=<>] [dest=<>]\nRestore the specified VM.",
        "list": "[rackname=<rackname>] [clusterless=yes|no] [exaOcid=<exaOcid>]\nGet the list of the current backedup vms. This operation will only return the list of local backups, for ossbackups use osslist",
        "osslist": "rackname=<rackname> \nGet the list of the current backedup vms. This operation will only return the list of os backups, for local backups use list",
        "download": "exaunitid=<exaunitid> vmname=<vmname> seq=<seq>\n Downloads an oss backup to the dom0",
        "restorelocal": "exaunitid=<exaunitid> vmname=<vmname> seq=<seq> [image=<u01|u02|grid|pv1_exadb|System> restartvm=<True|False>]\n Restores the specified file system (image) from the given backup sequence on a single VM (domu)",
        "restorepath": "<status uuid>\nReturns the location of the backup objects so user can manually restore them. This works for restorelocal and download operations",
        "configurecronjob": "hostname=<hostname> action=<enable/disable>\n Enables/disables vmbackup cronjob on the compute node\n",
        "suconfig": "user=<user> tenancy=<tenancy> fingerprint=<fingerprint> region=<region> [keyfile=<key_file>] [keycontent=<key_content>]\nCreates a superuser into ecra db to be used when using vm backup to oss",
        "patch": "rackname=<rackname> Patch vmbackup source code without changing current configuration"
    },
    "userconfig": {
        "create_user user=<username> password=<password> exaunit_id=<exaunit_id>": "\n Create a user (with a password).",
        "alter_user user=<username> password=<password> exaunit_id=<exaunit_id>": "\n Alter a user (with a password).",
        "grant_role role=<role> user=<username> exaunit_id=<exaunit_id>": "\n Grant a role to a user.",
        "list_user exaunit_id=<exaunit_id>": "\n List cell users.",
        "delete_user user=<username> exaunit_id=<exaunit_id>": "\n Delete a user.",
        "delete_role role=<role> exaunit_id=<exaunit_id>": "\n Delete a role."
    },
    "schedule": {
        "add": [
            "<job_desc_file>",
            "  Example:",
            "  {",
            "    \"job_class\": \"oracle.exadata.ecra.scheduler.CmdRunner\"",
            "    \"job_cmd\": \"ls /tmp\"  <-- give empty string if not required",
            "    \"job_params\": \"\"      <-- give empty string if not required",
            "    \"enabled\": \"N\"        <-- \"Y\" or \"N\"",
            "    \"interval\": 600         <-- seconds, multiple of 60",
            "  }",
            "",
            "  For custom-built job_class, implement 2 things below:",
            "  1. Make your class to implement JobInterface",
            "    e.g.",
            "      public class MyClass implements JobInterface {",
            "        ..",
            "  2. Implement run() method",
            "    e.g.",
            "      @Override",
            "      public void run(ScheduledJob job) {",
            "        // The argument 'job' is used for getting",
            "        // job_cmd and job_params.",
            "        .."
        ],
        "delete": "<job_id>",
        "list": "[<job_id> | <substring of job_class>]",
        "update": "<job_id> [<key>=<value>]*"
    },
    "diagnosis": {
        "rsyslog_get": "diagnosis rsyslog_get",
        "rsyslog_set": "diagnosis rsyslog_set target=exadata_log_to_lumberjack|otto value=<value> [otto_config_path=<otto_config_path>]\nSet rsyslog output config. When target is 'otto', the value should be either 'enabled' or 'disabled'",
        "logcol": [
            "\n  Usage:",
            "    diagnosis logcol id_name=<name> id_value=<value> [timestamp=<yyyy-mm-dd_HH24:MM:SS>] [hours=<hours>] [sosreport=true] [sundiag=true] [bugsr=<bugnum>|<srnum>] | regen_par=<tarfile_name>",
            "  Arguments:",
            "    id_name: rack_name, dbsystem_ocid, domu_hostname, cdb_ocid, pdb_ocid or db_unique_name",
            "    id_value: corresponding value",
            "    timestamp: yyyy-mm-dd_HH24:MM:SS in UTC. current time is used if not specified",
            "    hours: exacd_logcol will collect files modified after timestamp minus hours.",
            "           24 hours is used if not specified",
            "    sosreport: run sosreport. default is false",
            "    sundiag: run sundiag. default is false",
            "    bugsr: put bug num or sr num in tarball for better classification",
            "  Example:",
            "    diagnosis logcol id_name=rack_name id_value=sea201203exd-d0-05-06-cl-07-09-clu01",
            "    diagnosis logcol id_name=rack_name id_value=sea201203exd-d0-05-06-cl-07-09-clu01 sosreport=true",
            "    diagnosis logcol id_name=domu_hostname id_value=atpd-exa-kklm31.c3matchingmgmt.clientoverlappi.oraclevcn.com timestamp=2019-08-20_02:33:22 hours=48"
        ],
        "add_pre_logcol_rack": "name=<rackname>",
        "list_pre_logcol_rack": "",
        "delete_pre_logcol_rack": "name=<rackname>",
        "plgmon": "plgmnon",
        "add_ignore": "elastic_check pattern=<pattern>\nMake exacd ignore the targets whose names match the pattern.",
        "list_ignore": "elastic_check\nShow the list of the ignored exacd targets.",
        "delete_ignore": "elastic_check [pattern=<pattern>] [id=<id>]\nDelete an ignored exacd target. Either pattern or id should be specified.",
        "show_active_db_conn": "\nShow all active DB connections from ECRA.",
        "db_conn_stacktrace": "(show|enable|disable)\nIf enabled, all DB connections will keep stacktrace of the caller. (This change will be reset after restarting ECRA)",
        "logsearch": [
            "diagnosis logsearch ( request=<id> | workflow=<id> | workflow_task=<id> | exacloud_request=<id> | patch_crid=<id> ) [scope=(all|subrequests|single_request)]",
            "request, workflow, workflow_task, exacloud_request, and patch_crid are mutually exclusive",
            "'scope=all' (default option) will return all related request information. 'scope=subrequests' will return the target request and its child requests. 'scope=single_request' will not return any related request information"
        ],
        "rackhealth_run": "type=(cabinets|clusters) targets=targets [checks=<checks>]\nRun rackhealth tool for the given cabinets or clusters.\n'targets' comma-separated list of target cabinets or clusters.\n'checks' comma-separated list of items to check (e.g. netchk,sanitycheck,rdsping).",
        "rackhealth_result": "type=(cabinets|clusters) id=request_uuid\nGet the result of the previous rackhealth request, which could be run by 'diagnosis rackhealth_run' command."
    },
    "requests": {
        "abort exaunitid=<exaunit_id> operation=<operation> [status=<status>] [resourceid=<resource_id>]": "\n\tAbort a request given an exaunitid and an operation, if no status parameter is provided, the default status value for the aborted request is 500. (Note that this operation only cleans up the metadata in ECRA, if there is any operation in progress in ExaCloud it needs to be terminated manually)",
        "abort_async uuid=<uuid>": "\n\tAbort an async call setting its status to 500 so the status updater can terminate the async operation.",
        "list_async [status=<http_status>]": "\n\tGet a list of ecra async calls.",
        "multiop_details id=<request_id>": "\n\tList the children operations associated with parent request.",
        "multiop_recover id=<request_id>": "\n\tRecover a multi operation request based on the request id",
        "multiop_enable_step id=<request_id> step=<step>": "\n\t Enable the execution of the specified step",
        "multiop_disable_step id=<request_id> step=<step>": "\n\tDisable the execution of the specified step",
        "update": "request_id=<request_id> [status=<status>] [operation=<operation>] [errors=<errors>] [target_uri=<target_uri>] [details=<details>] [body=<body>] [resource_id=<resource_id>] [status_uuid=<status_uuid>] \n\tUpdate params of a request based on the request id",
        "info [id=<request_id>] [zone=<zone>] [id=<request_id>] [operation=<operation>] [status=<request_status>] [exaunitid=<exaunit_id>] [rackname=<rackname>]": "\n\tGet a list of requests based on the parameters provided. If no parameters are given all requests are returned.",
        "get": "id=<request_id> [fields=<comma splitted fields to retrieve>]\n\tGet requests data. If fields option is provided the info related to those fields will be retrieved.",
        "addregistry": "requestid=<request_id> rackname=<rackname> operationname=<operation_name> [resourceid=<resource_id>]\n\t Creates a new record on ECS_REGISTRIES for a request that was previously aborted."
    },
    "inventory": {
        "register": "<inventoryId> hw_type=<cell|compute>\n Register an inventory hardware in ECRA.",
        "get": " hw_type=<cell|compute> [params=value]* \n Get inventory info using capacity tables using query details.",
        "deregister": "<inventoryId>\n Deregister inventory from ECRA metadata, Note: must be elegible for deletion.",
        "get_hardware": " [status=<status> | model=<model> | hw_type=<COMPUTE|CELL>] | fabric_name=<value> | servicetype=<value> | <param=value>\n Get inventory hw info (CC and capacity) from ECRA and provide details of components",
        "reviewcapacity": " [status=<status> | model=<model> | hw_type=<COMPUTE|CELL>] | fabric_name=<value> | servicetype=<value> | <param=value>\n Get true/false depending if there's hw info  for given parameters spec",
        "getexcludelist": "<cabinetname=name>\n Get the list of non-free nodes for the given cabinet",
        "update": "<inventoryId> <params=value>* | specs_path=path \n Update information for given inventory, using space separated key=value list.",
        "update_hwnode": "<hostname> [<key>=<value>]* \n Update data in the ECS_HW_NODE for the given hostname \n For key model_subtype only valid values are ('ELASTIC_LARGE','STANDARD','ELASTIC_EXTRALARGE')",
        "reserve": "<json_path=filepath>\nThe JSON file should contain the next parameters:\n\tMandatory parameters: rackname=<rackname>, exadataInfrastructureId=<exadataInfrastructureId>\nMay contain the next optional parameters:\n\t[server=<server>], [hostnames=<hostnames>], [idemtoken=<idemtoken>], [shape=<shape>], [model=<model>] \n Used to reserve inventory for elastic flow",
        "release": "json_path=<filepath> | hw_type=<COMPUTE|CELL> [hostname=<host> | quantity=<# of servers> rackname=<rackname> exadataInfrastructureId=<exaocid>]\n The JSON file should contain the servers infomation\n Used to release inventory during elastic flows",
        "check": "[target_nodes=<all|node names>] [update_node_state=<true|false>]\n  target_nodes can be 'all' or comma-separated list of oracle_hostnames. Default parameters are target_nodes=all update_node_state=true.\n Used to check health of the compute/cell nodes in elastic pool",
        "update_node_detail": "cabinetname=<cabinetname> | hostnames=<hostname1, hostname2>| rackname=<rackname>| allnodes=<true|false>\n Recover from exacloud admin ether and serial number\n Hostnames list correspond to cells OR computes\n allnodes will do a backfill update on all nodes\n When using hostnames be sure that the nodes are kvm nodes.",
        "get_node_detail": "cabinetName=<cabinetname> | hostnames=<hostname1, hostname2>\n Get details on computes and cells including serial number and admin ether.",
        "updatenodes": "<listofnodes=comma separated node list> [<key>=<value>]* \n Update the data for the given list of hosts in ECRA DB\n",
        "resetcavium": "caviumid=<caviumid> [ignorenodestate=<true|false> action=<stop|start|reset> reset by default] targetdevice=<ilom|dom0>  \nResets T93 slot of given cavium or stop interface \n",
        "caviumcollectdiag": "caviumid=<caviumid> | hostname=<hostname> \nGets diagnostic information about a T93 slot \n",
        "caviumdiagresponse": "clobid=<clobid> \nGets response from caviumcollectdiag endpoint \n",
        "resetvlan": "cells=<hostname1,hostname2,..> (rack=<rack> | cabinet=<cabinet>)\nReset VLAN ID of cell nodes\n",
        "startblackout": "<hostname=name>\n Initiate a blackout on the host\n",
        "endblackout": "<hostname=name>\n End a blackout on the host\n",
        "getspec": "inventoryid=<inventory_id=> rackname=<rack_name>\n Gets exadata formation spec of inventory id in given rack\n",
        "summary": "[nodestate=<nodestate>]\n Print a easy-to-visualize summary of all nodes having the given state. If nodestate is left empty, nodes with any state will be returned\n",
        "updatestatuscomment": "component=<node|cabinet> name=<component name> content=<comment content>\n Updates the field status_comment of the indicated component.",
        "getstatuscomment": "component=<node|cabinet> name=<component name>\n Gets the contents of the field status_comment of the indicated component.",
        "resetilompassword": "listofnodes=<node name1, node name2>\nReset the ilom pwd to default one on the desire node(s)."
    },
    "exadatainfrastructure": {
        "create": "<json_path=filepath>\n The JSON file containing exadataInfrastructureId, idemtoken and servers information.\n Used to reserve nodes for elastic shape CEI\n",
        "get": "[exadataInfrastructureId=ocid] [filter=<computes|servers|rackslots|allocations>] [rackname=<RACKNAME>]\n Get the rackname & node information for the CEI OCID\n",
        "delete": "exadataInfrastructureId=ocid\n Delete the CEI/release the nodes associated with the OCID\n",
        "rack_reserve": "exadataInfrastructureId=<exainfraocid> [nodeComputeAliases=<list-of-aliases>] [vmclusterid=<vm-cluster-id>] [rackname=<rackname>] [provisionType=<preprovision>]\nexainfraocid   : OCID for the CEI list-of-aliases: List of aliases, in comma separated. vmclusterid: ocid from CP. Create an slot for the given ocid and based on the compute aliases, if there is not provided a list of aliases all the aliases tha belongs to the infra will be used. rackname: The name of the rack that you want to reserve",
        "rack_release": "exadataInfrastructureId=<exainfraocid> rackname=<rackname>\nexainfraocid   : OCID for the CEI rackname       : Name of the slot that will be released. Relase the rackname for the given exainfraocid",
        "add_cluster": "exadataInfrastructureId=<infra ocid> rackname=<slot name> json_path=<payload path> [dev=true] [cores=<CORES> memorygb=<MEMORY> ohomesizegb=<OHSIZE> storagetb=<STORAGETB> gridversion=<grid_version>] [diskgroupsallocation=<D:R:S>] [createsparse=<true/false>] [backupdisk=<true/false>]\nCORES : #of Cores, MEMORY : Memory in GB, OHSIZE: Oracle home in GB, STORAGETB : Storage in TB, gridversion: the grid version to use in the cluster (example 19).",
        "attachstorage": "exadataInfrastructureId=<infra ocid> [asmpowerlimit=<value of asm>] [skipresize=<true|false>]\nAdd Reserved Cells to the given Infrastructure",
        "deletestorage": "exadataInfrastructureId=<infra ocid> servers=<cells to delete separated by commas> [releaseservers=<true|false>][asmpowerlimit=<value of asm>]\nDelete Cells to the given Infrastructure",
        "checkdataintegrity": "exadataInfrastructureId=<infra ocid> [onlyvalidations=true]\nProvides a check of the metada information",
        "reshapecluster": "exadataInfrastructureId=<exainfraocid> exaunitId=<exaunitId> [cores=<CORES> memoryGb=<MEMORY> ohomeSizeGb=<OHSIZE> storageTb=<STORAGETB> filesystem=<[montpoint:size, etc.]> ] [diskgroupsallocation=<D:R:S>] [createsparse=<true/false>] [backupdisk=<true/false>]\nexainfraocid   : OCID for the CEI , exaunitId: exaunitid of the cluster CORES : #of Cores, MEMORY : Memory in GB, OHSIZE: Oracle home in GB, STORAGETB : Storage in GB. Note: The provided values should be provided per node, and this quantity will be increased per node for all the nodes that belong to the exaunit provided. The only resource that is increased at exaunit level is storageTb. FILESYSTEM: For filesystem reshape is required a comma separated array of [name of the mountpoint: size in GB]. e.g. filesystem=/u01:50,/var:15",
        "reshapecluster_precheck": "exadataInfrastructureId=<exainfraocid> exaunitId=<exaunitId> [cores=<CORES>]\nexainfraocid   : OCID for the CEI , exaunitId: exaunitid of the cluster CORES : #of Cores. Note: The provided values should be provided per node, and this quantity will be increased per node for all the nodes that belong to the exaunit provided. ",
        "restorecluster": "exaunitId=<exaunitId> exadataInfrastructureId=<exainfraocid> requestId=<requestId>. Run in case reshapecluster attempts fail. First, execute this operation and once it finishes correctly, performs the workflows abort workflowId=<workflowId> command",
        "migratetomvm": "exadataInfrastructureId=<infra ocid> rackname=<infra rack> precheckonly=true|false [vmclusterocid=<cluster OCID>] [tenancyocid=<Tenancy OCID>]\nMigrate the specified SVM infrastructure to a MVM infrastructure",
        "info": "Show a summary of all the Infrastructures creted in this environment",
        "computevms": "oraclehostname=<ORACLEHOSTNAME> The API will return the VMs for the provided oracle hostname",
        "drop:": "rackname=<RACKNAME> exadataInfrastructureId=<EXADATAINFRAID> API that forecfully cleans up remaining metadata after a failed MVM operation that requires metadata deletion.",
        "getinitialpayload": "rackname=<rackname> | ceiocid=<ceiocid>. Returns the create infra payload",
        "recoverclunodes_sop": "exadataInfrastructureId=<exainfraocid> jsonconf=<jsonconf_filepath>\nPerforms the node recovery sop over the provided exadataInfrastructureId",
        "dropclunodes_sop": "exadataInfrastructureId=<exainfraocid> jsonconf=<jsonconf_filepath>\nPerforms the drop nodes for node recovery sop over the provided exadataInfrastructureId",
        "secureerase": "exadataInfrastructureId=<exainfraocid> idemtoken=<uuid> [cellnodes=<cell1hostname,cell2hostmame>]\nExecute Secure Erase on all cells of the infra, this operation requires extra configuration at exacloud level, this operation is not reversable be careful. cellnodes provides an optional approach to send specific cell nodes spearate the hostnames with commas.",
        "getsecureerasecert": "exadataInfrastructureId=<exainfraocid> [path=<filePath> operationtype=<secure_erase|infra_delete|storage_delete> cellnode=<cellnodes> downloadtype=<file|direct|ossfile|ossdirect>]\nAfter secure erase is completed use this to get the certificates, specify a file location using path it could have a file name or just a directory, by default file is saved in ecracli folder.\nUsing OSSFile or OSSDirect will try to download certificates from the OSS, GENERATE_URL will create a PAR url",
        "disablebackup": "exadataInfrastructureId=<exainfraocid> increasefactor=<factor to increase available storage>.\n Disables te restriction of available storage used for backups, increasing the space that is consumed by filesystem and oracle home. The default size is 2243 GB, the increase factor will multiply that size by the indicated number. E.g. 2243 * increasefactor=3 = 6729 ",
        "getkeys": "<exainfraocid> [host=<hostname of machine/all>] [user=<username for which you need key root/oracle>] [nodetype=<dom0/domu/ibswitch/cell/all_nodes>]. Get the SSH key information."
    },
    "formation": {
        "delete": " formation_id=<id> inventory_id=<id>  \n Deletes a record from the Exadata Formation Table.",
        "list": " [formation_id=<id>]\n Show all associated hardware on this exadata formation or all formations available."
    },
    "exacc": {
        "createNetwork": "idemtoken=<idemtoken> json_path=<file path> \n Create a new network",
        "listNetworks": "exa_ocid=<exadata ocid> [status=<status>] \n List all the networks of the specified exadata ocid, if the status parameter is provided the list if filtered by the status. ",
        "updateNetwork": "idemtoken=<idemtoken> json_path=<file path> \n Update the network specified on the payload.",
        "getNetwork": "network_ocid=<networkocid> \n Get all the information about the specified network",
        "deleteNetwork": "network_ocid=<network ocid>  \n Delte the specified network",
        "validateNetwork": "idemtoken=<idemtoken> network_ocid=<network ocid> \n Execute the network validation process.",
        "networkValidationReport": "network_ocid=<network ocid> \n Get the network validation error report.",
        "activate": "idemtoken=<idemtoken> exacc_ocid=<exacc ocid> activation_file=<file path> \n Execute the ExaCC activation process.",
        "certificateRotation": "rotationType=<[CLIENT, VPN, EXACLOUD, REMOTEMANAGER, DBCP, CPSPROXY, VPNPROXY]> \n Execute the certificate rotation depending on the rotationType.",
        "secretServiceRotation": "exacc secretServiceRotation rotationType=<rotationType> exaOcid=<exaOcid> \n Generate a new password and update the secret",
        "secretServiceCompartment": "exacc secretServiceCompartment \n Get the compartment Id and vault Id for the secret service"
    },
    "ocitenants": {
        "list": " \n List the OCI Exadata Tenants",
        "capacity": " page=<pageIndex> pageSize=<pageSize> [tenantOcid=<ocid>] \n List the capacity of all the tenants. Will list for specific tenant if tenantOcid is supplied",
        "fleetcapacity": " page=<pageIndex> pageSize=<pageSize> \n List the basic capacity attributes at fleet level(of all the tenants)"
    },
    "ocicapacity": {
        "register": " jsonconf=<jsonconf_filepath> [idemtoken=<idemtoken>] \n Ingest a new OCI capacity (Exadata) with given properties",
        "details": " exaOcid=<ocid> \n List the details of OCI Exadata",
        "update": " exaOcid=<ocid> jsonconf=<jsonconf_filepath> [idemtoken=<idemtoken>] \n Update modifiable properties of OCI Exadata with given ocid specified in supplied JSON",
        "delete": " exaOcid=<ocid> \n Delete the OCI Exadata with given ocid from ECRA",
        "reserve": " exaOcid=<ocid> [idemtoken=<idemtoken>] \n Reserve the OCI Exadata in Pre-Activation state with given ocid in ECRA",
        "getconfigbundle": " exaOcid=<ocid> saveloc=<cfgbundle_filepath> \n Save the config bundle of OCI Exadata with given ocid generated by Pre-Activation action to specified location and filename",
        "getcfgbundlecksum": " exaOcid=<ocid> \n Get the SHA-256 checksum of the config bundle of the OCI Exadata with given ocid generated by Pre-Activation action",
        "getprivkey": " exaOcid=<ocid> saveloc=<privatekey_filepath>\n Get the SSH private key to connect to OCPS of OCI Exadata with given ocid generated by Pre-Activation action",
        "getcfgbundlepasswd": " exaOcid=<ocid> saveloc=<cfgbundlepassword_filepath>\n Get the Cipher password to decrypt the encrypted config bundle of the OCI Exadata with given ocid generated by Pre-Activation action",
        "migrateToMvm": " exaOcid=<ocid> [idemtoken=<idemtoken>] \n Migrate the given Single VM OCI Exadata with given ocid in ECRA",
        "enableNs": " exaOcid=<ocid> [idemtoken=<idemtoken>] \n Enable node subset feature for OCI Exadata with given ocid in ECRA",
        "enableElasticCell": " exaOcid=<ocid> [idemtoken=<idemtoken>] \n Enable elastic storage feature for OCI Exadata with given ocid in ECRA",
        "enableElasticCompute": " exaOcid=<ocid> [idemtoken=<idemtoken>] \n Enable elastic compute feature for OCI Exadata with given ocid in ECRA",
        "release": " exaOcid=<ocid> rackSlot=<rackName> [idemtoken=<idemtoken>] \n Release specific rack in 'RESERVED' state identified by 'rackSlot' of OCI Exadata with given ocid",
        "updateInfraStatus": " exaOcid=<ocid> status=<Status> \n Update infra status ",
        "restoreAllocations": " exaOcid=<ocid> exaunitId=<exaunitId> [idemtoken=<idemtoken>] \n Restore reserved allocations for any operation against given exaunitId of the exaOcid",
        "fixXmlActionTag": " exaOcid=<ocid> \n Fix updated XMLs to save value of deployed attr as true for PROVISIONED clusters of the exaOcid",
        "reserveInfra": " exaOcid=<ocid> \n Mark exadata infra identified by given exaOcid as RESERVED ",
        "releaseInfra": " exaOcid=<ocid> \n Mark exadata infra identified by given exaOcid as READY ",
        "undoExascale": " exaOcid=<ocid> \n Revert the exascale configuration in ECRA metadata and Exacloud",
        "updateAsmss": " exaOcid=<ocid> asmss=<true|false> \n Update ASMSS config for the infra",
        "getnatmap": " exaOcid=<ocid> \n List the natdomumap of OCI Exadata. vmClusterOcid is optional",
        "attachCellServersExascale": " exaOcid=<ocid> \n Execute the Exascale cell attachment workflow on the given exaOcid",
        "markCellsToDelete": " exaOcid=<ocid> cellList=<cells separated by comma> \n Mark a list of cells to be deleted"
    },
    "workflows": {
        "list": " [workflowStatus=Initialized|Ready|Processing|Failed|Completed|Aborted] [pageSize=<pageSize>] [pageIndex=<pageIndex>] [exaunitId=<exaunitid>] [workflowId=<workflowId>]\n  Returns workflows list. Optional filter criteria field are workflowStatus, pageSize, pageIndex, exaunitId, workflowId.\n  pageSize signifies number of workflows to be listed per page and pageIndex signifies the page number/index.\n     Example: workflows list workflowStatus=Failed pageSize=5 pageIndex=1\n      i.e. display 1st page of size 5 of failed workflows.",
        "describe": " workflowId=<workflow's uuid>\n  Returns the detailed workflow information associated to given workflow id.\n  Example: workflows describe workflowId=2b08558b-2622-4769-b597-829e3b991997",
        "undo": " workflowId=<workflow's uuid> taskName=<taskName>\n  Submits a 'undo' operation for the given task, returns operation's uuid for request submitted.\n  Possible task names can be found with workflows describe command.\n  Example: workflows undo workflowId=2b08558b-2622-4769-b597-829e3b991997 taskName=CreateStorage.",
        "retry": " workflowId=<workflow's uuid> taskName=<taskName>\n  Submits a 'retry' operation for the given task, returns operation's uuid for request submitted.\n  Possible task names can be found with workflows describe command.\n  Example: workflows retry workflowId=2b08558b-2622-4769-b597-829e3b991997 taskName=CreateStorage.",
        "complete_task": "workflowId=<workflow uuid> operationId=<operation uuid> [force=<true|false>] [task_output_path=</path/to/json>]\n Submits a markTaskComplete over the given operation UUID. By default the input payload will be forwarded as output, unless an output payload is provided.",
        "rollback_mode_on": " workflowId=<workflow's uuid>\n  Enables rollback mode for a given workflow. Workflow instance tasks won't be executed automatically.\n  Manual rollback, undo and retry operations can be performed in this mode.\n  Workflow need to be set back with rollback_mode_off in order to get the workflow completed.\n  Example: workflows rollback_mode_on workflowId=2b08558b-2622-4769-b597-829e3b991997",
        "rollback_mode_off": " workflowId=<workflow's uuid>\n  Disables rollback mode for a given workflow and now the workflow is in normal mode.  Enables workflow instance tasks to be auto executed.\n  Example: workflows rollback_mode_on workflowId=2b08558b-2622-4769-b597-829e3b991997",
        "rollback": " workflowId=<workflow's uuid>\n  moves undo pointer to previous task of current undo task pointer.\n  This command works for a workflow when its rollback mode is on.\n  When rollback_mode_on is set, the undo pointer points to last task by default.\n  Example:workflows rollback workflowId=3ea84e40-f33a-4f82-a49b-df47e64f4d64",
        "operation": " operationId=<operation's uuid>\n  Returns the operation (undo, retry) status details.\n  Example: workflows operation operationId=a36acb36-63fe-4587-abc5-d9e364bbebdb",
        "abort": " workflowId=<workflow's uuid> [force=true]\n Submits an 'abort' operation for the given workflow, workflow must be in the failed state.\n Once abort is done, manual cleanup will be required.\n Example: workflows abort workflowId=2b08558b-2622-4769-b597-829e3b991997.\n To force abort the WF use option 'force=True' in addition",
        "cancel_task": " workflowId=<workflow's uuid> taskName=<taskName>\n  Submits a 'cancel_task' operation for the given task, returns operation's uuid for request submitted.\n  Possible task names can be found with workflows describe command.\n  Example: workflows cancel_task workflowId=2b08558b-2622-4769-b597-829e3b991997 taskName=CreateStorage.",
        "fail_task": " workflowId=<workflow's uuid> taskName=<taskName> operationId=<operationId> \n  Submits a 'failtask' operation for the given task, returns operation's uuid for request submitted.\n  Possible task names can be found with workflows describe command.\n  Example: workflows fail_task workflowId=2b08558b-2622-4769-b597-829e3b991997 taskName=CreateStorage operationId=6b08558b-2622-4769-b597-829e3b991997.",
        "server_status": "Gets name and status of servers from wf_server table.\n Can be used to check if an EcraServer is 'RUNNING' or 'FAILING OVER'\n Example: workflows server_status",
        "reload": "Reload workflow from old ecra server\n Example: workflows reload oldserver=EcraServer1, oldserver value can be EcraServer1/EcraServer2",
        "pause": "Pause all workflows in the old ecra server\n Example: workflows pause value=true, value can be true/false",
        "janitor_restart": "Restart WF janitor preferrably after setting WF_JANITOR_TASK_DEFAULT_INTERVAL_SECONDS ecs property",
        "exacloud_success": " workflowId=<workflow's uuid> operationId=<operationId> \n  Submits a 'exacloud_success' operation for the given operationid.\n  Possible operationId can be found with workflows describe command.\n  Example: workflows exacloud_success workflowId=2b08558b-2622-4769-b597-829e3b991997 operationId=6b08558b-2622-4769-b597-829e3b991997.",
        "getinput": " workflowId=<workflow's uuid> [taskname=<taskname>] \n  Retreives the input payload for the current task of a workflow ans saves it on the filesystem. Provide only the workflowId to retrieve the initial payload",
        "updateinput": " workflowId=<workflow's uuid>  [taskname=<taskname>] jsonpath=<jsonpath>\n  Retreives an input payload from the filesystem and updates it on the current task of a workflow. Provide only the workflowId to update the initial payload"
    },
    "ocicpinfra": {
        "savevpnserverdetails": " jsonconf=<jsonconf_filepath> \n Save the VPN server details for an AD in the region into ECRA DB",
        "getvpnserverdetails": " ad=<ad> \n Fetch the VPN server details for the given AD in the region from ECRA DB",
        "updatevpnserverdetails": " jsonconf=<jsonconf_filepath> \n Update the VPN server details for an AD in the region in ECRA DB",
        "rotatecipherpasswd": " Rotate the Cipher password for ECRA instance",
        "getecracipherpasswd": " saveloc=<ecraccipher_password_filepath> \n Get current Cipher password active in the target ECRA instance",
        "geteallcipherpasswds": " saveloc=<ecraccipher_password_filepath> \n Get all Cipher passwords in the target ECRA instance",
        "registervpnhe": " jsonconf=<jsonconf_filepath> \n Save the VPN HE details into ECRA DB",
        "updatevpnhe": " ip=<vpn_he_ip_address> jsonconf=<jsonconf_filepath> \n Update the existing VPN HE details identified by the ip into ECRA DB",
        "getvpnhedetails": " ip=<vpn_he_ip_address> \n Get the VPN HE details identified by the ip",
        "getvpnhelist": " ad=<ad> \n Get the VPN HE details of all registered ones in the ad"
    },
    "heartbeat": {
        "get": "\n List all heartbeat data for the current OCI Exadatas on ECRA.",
        "exadata": "tenant_ocid=<tenant_ocid> exadata_ocid=<exadata_ocid>\n Retrieve heartbeat data for a given OCI Exadata.",
        "enableScheduler": "heartbeatEnabled=<heartbeatEnabled>\n Change the Heartbeat Scheduler status [true|false], if set to 'true', heartbeat job will keep running, if set to 'false' jobs will stop running."
    },
    "profiling": {
        "report": "[rackname=<RACKNAME>][start_date=<YYYY-MM-DD>] [end_date=<YYYY-MM-DD>] [interval=<DAYSINTERVAL>] [alloperations=<true/false>]\n If rackname is given, tool will profile only timings for that rack, else will profile all racks. Other parameters are mutually exclusive - if start/end date are provided, interval should not be provided and vice versa",
        "operation": "taskId=<OPERATION_NAME> [subtask=<true|false>] [start_date=<YYYY-MM-DD>] [end_date=<YYYY-MM-DD>] [interval=<DAYSINTERVAL>] [alloperations=<true/false>]\n It will give a report for specific given operations. For example, timmings for create-starter-db on ecs_request table.  Other parameters are mutually exclusive - if start/end date are provided, interval should not be provided and vice versa is you include the subtask argument and the value is true the result show a detail of sub-task",
        "infrastructure": "[ceiocid=<EXAOCID>][start_date=<YYYY-MM-DD>] [end_date=<YYYY-MM-DD>] [interval=<DAYSINTERVAL>] [alloperations=<true/false>]\n It will give a report for operations to into infrastructure. For example, total requests, successes, inprogress as well as failed count along with success/failure rates.  Other parameters are mutually exclusive - if start/end date are provided, interval should not be provided and vice versa"
    },
    "compliance": {
        "scan": "(show|enable|disable) [host=(all|hostname)] [rack=rackname] [host_type=(dom0|domu|cell)]\nEnable or disable AEP/CM scan for the host, or show current scan setting",
        "default_scan": "(show|enable|disable)\nSet default scan status. If the default is enabled, when a new rack is registered, AEP/CM scan will be automatically enabled",
        "base_cfg": "update justification=justification [host=(all|hostname)] [rack=rackname] [host_type=(dom0|domu|cell)]\nSet baseline configuration from current system configuration",
        "cfg_status": "show host=hostname cfg_type=(all|config types)\nShow current configuration status of a host",
        "cfg_history": "list host=(all|hostname) [search_from=from_time] [search_to=to_time]\ncompliance cfg_history show change_id=id\nShows the configuration change history (including baseline config change). List command will show only the metadata of changes, and show command will show the configuration content as well. The format of from_time/to_time is ISO8601, e.g. 2019-07-31T20:13:31Z or 2019-07-31T13:13:31-07:00",
        "report_oss_namespace": "show\ncompliance report_oss_namespace set value=NAMESPACE\nShow or set Object Storage namespace for AEP/CM reporting",
        "auto_revert": "(show|enable|disable)\nEnable or disable auto-revert",
        "status": "show\nShow all non-compliant statuses",
        "avfim": "run [host=(all|hostname)] [rack=(all|rackname)] [component=(all|av|fim)]\nRun AV/FIM on specific host or rack"
    },
    "ecspatchversion": {
        "list": " rackname=<rackname>  \n List the available and applied versions of exadata and cps services"
    },
    "ecra": {
        "validateApis": "<json_path> \n Validate health of ECRA installation according to json configured on given testJSon ",
        "dumpConfig": "<target=[ecracli]> \n Command to dump ecra config in json format, param indicate which config should be dumped",
        "coredump": "action=<ACTION> coredumppath=<CORE_DUMP_PATH> exaunitId=<EXAUNIT_ID>\n Valid options for <ACTION> are (setup|detach|dumpcore|uploadimagefile), valid options for <CORE_DUMP_PATH> are (local|mount_target) a mount_target example is 10.0.0.3:\\/FileSystem-389274",
        "upgradeHistory": "[fromDate=<MMM-DD-YYYY>]\n Return dbaas, exacloud, oeda and ecra war upgrade versions from the specified date. If fromDate parameter is omitted it returns upgrades of the last 6 months",
        "getfile": "<filename> \n Retrieves and downloads the specified file from the ECRA_FILES table in the database to the PodRepo Directory",
        "deletepayloads": "olderthan=<DATE|INTERVAL> [onlycheck=<true|false>]\nDeletes all files that are older than the provided date or interval. Interval is expected to be 1M-12M 1D-30D 1Y-9Y, if using date it is expected to use this format: YYYY-MM-DD HH24:MI:SS. By default onlycheck is true, so no file will be removed.",
        "connect": "<hostname=hostname to connect>.\n Connect ECRACLI to the specified ECRA host. All the following operations will be executed using the new host.",
        "updatefile": "filename=<filename> contentpath=<path of file> \n Update the indicated file using the content found on the specified path.",
        "listfiles": "[rackname=<rackname>|exaunitid=<exaunitid>][maxfiles=<number of files to display>]\n Retrieves the indicated number of files, by default 10, using either the rackname or the exaunitid."
    },
    "ecralogs": [
        "<target_path> [-zip] <requestId> \n Returns available internal logs for the target ecra instance to the given folder"
    ],
    "exadata_applied_version": {
        "list": " rackname=<rackname> get the list of applied image version for exadata infra components"
    },
    "metadata": {
        "tables": "Show the list of allowed tables",
        "select": "table=<TABLE> select=<COLUMS> where=<CONDITION> values=<VALUES>\n Select the values given in the select argument for the table specified for the conditions in the where argument",
        "update": "table=<TABLE> set=<CONDITIONS> \n Update the values given in the set argument for the table and PK specified."
    },
    "opctl": {
        "create_user": "<json_path> \n Create an access restricted user according to json configuration",
        "delete_user": "<json_path> \n Delete an access restricted user according to json configuration",
        "assign": "<json_path> \n Assign policy according to json configuration"
    },
    "exacloud": {
        "setexassh": "rackname=<rackname> \n Name of the rackname to update the exassh table "
    },
    "analytics": {
        "analyze": "operation=<OPERATION> [interval=<MINUTES>]",
        "history": "[rackname=<RACKNAME>] [inventoryid=<INVENTORYID>] [exadatainfrastructure=<EXADATAINFRASTRUCTURE>] [vmclusterid=<Cluster OCID>] [start_date=<YYYY-MM-DD>] [end_date=<YYYY-MM-DD>] [interval=<DAYSINTERVAL>] \n if start/end date are provided, interval should not be provided and vice versa. Also, rackname, inventoryid or exadatainfrastructure should be provided, not both, and at least one of them.",
        "stats": "operation=<OPERATION>[start_date=<YYYY-MM-DD>] [end_date=<YYYY-MM-DD>] [interval=<DAYSINTERVAL>] \n if start/end date are provided, interval should not be provided and vice versa",
        "csconfigcheck": "exaunitid=<EXAUNITID>[[mandatory=<MANDATORY> result=<RESULT>] MANDATORY: Filter in case check is mandatory or not. Valid values (true or false), RESULT: Filter checks result. Valid values (pass or error)]",
        "rack": "rackname=<RACKNAME> [operation=<OPERATION>]\n Get all the operations associated with the provided rackname",
        "csconfigtemplate": "rackname=<RACKNAME> \n Get the postcheck template for the provided rackname",
        "getoperationdetails": "operation=<OP_NAME> [start_date=<START_DATE> end_date=<END_DATE>]\nGet the full details of every operation matching the OP_NAME parameter. Using both the start_time and end_time parameters will throw errors.",
        "getpayload": "id=<ANALYTICS_ID | IDEMTOKEN>\nGet the payload from operation matching the provided id parameter. To retrieve the id from analytics use the <analytics info> command before. Idemtoken value can also be used to retrieve the payload.",
        "info": "[rackname=<RACKNAME>] [exaunitid=<EXAUNITID>] [exadatainfrastructure=<EXADATAINFRASTRUCTURE>] [vmclusterid=<Cluster OCID>] [start_date=<YYYY-MM-DD>] [end_date=<YYYY-MM-DD>] [interval=<DAYSINTERVAL>] \n if start/end date are provided, interval should not be provided and vice versa. Also, exaunitid, rackname or exadatainfrastructure should be provided, just one of them.\nResult: the command retrieves records from analytics that match the provided parameters"
    },
    "patchcps": {
        "cpsswupgrade": "cps_payload= <payload path> \n CPS SW patching payload file path",
        "cpssw_tar_upload": "cps_tar_payload= <payload path> \n CPS SW patching pre/post upgrade tar json file path"
    },
    "patchmetadata": {
        "registerlaunchnodes": [
            "\n  Usage:",
            "  infraName=<Name of the infra> infraType=<Type of infra CLUSTER|CABINET|QFAB|SINGLE_VM_CLUSTER> launchNodes=<comma seperated list of launch nodes>",
            "  Description: Register launchnode for an infra to perform infra patching",
            "  Arguments:",
            "    infraName(Optional): Name of the infra",
            "    infraType(Mandatory): Type of infra CLUSTER|CABINET|QFAB|SINGLE_VM_CLUSTER",
            "    launchNodes(Mandatory): comma separated list of launch nodes",
            "    launchNodeType(Optional): Valid values are MANAGEMENT_HOST and COMPUTE. This attribute is optional in case of SINGLE_VM_CLUSTER and it will be defaulted to MANAGEMENT_HOST. For clusterless cabinets it will be defaulted to COMPUTE",
            "  Example:",
            "    patchmetadata registerlaunchnodes infraName=slcs27 infraType=CLUSTER launchNodes=slcs27adm03,slcs27adm04",
            "    patchmetadata registerlaunchnodes infraType=SINGLE_VM_CLUSTER launchNodes=slcs27adm03",
            "    patchmetadata registerlaunchnodes infraName=cabinet_name infraType=CABINET launchNodes=FQDN_of_Launchnode launchNodeType=COMPUTE",
            "    patchmetadata registerlaunchnodes infraName=qfab_name infraType=QFAB launchNodes=FQDN_of_Management_Host launchNodeType=MANAGEMENT_HOST",
            "    patchmetadata registerlaunchnodes infraName=cabinet_name infraType=CABINET launchNodes=FQDN_of_Management_Host launchNodeType=MANAGEMENT_HOST"
        ],
        "deregisterlaunchnodes": [
            "\n  Usage:",
            "  infraName=<Name of the infra>",
            "  Description:Deregister launchnode of an infra for infra patching, this operation will delete the launch",
            "              node registered from ECRA Db",
            "  Arguments:",
            "    infraName(Optional): Name of the infra",
            "    infraType(Mandatory): Type of infra CLUSTER|CABINET|QFAB|SINGLE_VM_CLUSTER",
            "    launchNodes(Optional): Comma seperated list of launch node names",
            "  Example:",
            "    patchmetadata deregisterlaunchnodes infraType=SINGLE_VM_CLUSTER",
            "    patchmetadata deregisterlaunchnodes infraName=slcs27 infraType=SINGLE_VM_CLUSTER",
            "    patchmetadata deregisterlaunchnodes infraName=slcs27 infraType=SINGLE_VM_CLUSTER launchNodes=slcs27adm03,slcs27adm04",
            "    patchmetadata deregisterlaunchnodes infraName=slcs27 infraType=SINGLE_VM_CLUSTER launchNodes=slcs27adm03",
            "    patchmetadata deregisterlaunchnodes infraType=SINGLE_VM_CLUSTER launchNodes=slcs27adm03,slcs27adm04",
            "    patchmetadata deregisterlaunchnodes infraType=SINGLE_VM_CLUSTER launchNodes=slcs27adm03"
        ],
        "getlaunchnodes": [
            "\n  Usage:",
            "  [infraName=<Name of the infra>] [infraType=<Type of infra CLUSTER|CABINET|QFAB|SINGLE_VM_CLUSTER>]",
            "  Description:Get the registered launch node for an infra",
            "  Arguments:",
            "    infraName: Name of the infra",
            "    infraType: Type of infra CLUSTER|CABINET|QFAB|SINGLE_VM_CLUSTER",
            "  Example:",
            "    patchmetadata getlaunchnodes infraName=slcs27 infraType=CLUSTER",
            "    patchmetadata getlaunchnodes infraName=slcs27",
            "    patchmetadata getlaunchnodes"
        ],
        "registerpluginscript": [
            "\n  Usage:",
            "  PluginTarget=<Name of the PluginTarget dom0|cell> PluginType=<PluginType exacloud|oneff> ScriptAlias=<Alias name for the script to be executed>  ScriptName=<Script Name> ScriptBundleHash=<ScriptBundle Checksum> ChangeRequestId=<ChangeRequestId> Description-<Brief description about the script>",
            "  Description: Register a plugin script for a particular PluginTarget with a specific PluginType with all other metadata about the script",
            "  Arguments:",
            "    PluginTarget: PluginTarget determines where the plugins are going to be executed, it can be dom0 or cell",
            "    PluginType: Execution context of the script. It can be exacloud or oneoff.",
            "    ScriptAlias: Represents simplified or alternative form of the script name. This alias name is used for modifying or deleting script metadata.",
            "    ScriptName: Name of the script to be executed which include relative path from standard exacloud plugin scipts location.",
            "    ScriptBundleHash: The SHA256 hash of the tarball that contains the script, associated artifacts, and a checksum file.",
            "    ChangeRequestId: The ChangeRequestID created for the process of registering the script.",
            "    Description: This is to provide comprehensive details about the script, including a brief description of its functionality. Additionally, include Ownership details to capture information about the script???s owner details such as component owner or team distribution list(DL).",
            "    IsEnabled: This is to determine whether the script still required to be executed in the environment, with this attribute we can temporarily disable a script to be executed. Default value is Yes.",
            "    FailOnError: This is to determine whether to stop infra patching flow in case of script execution failure. Default value is Yes.",
            "    Phase: Refers to the phase during which the script is executed. It can be pre, post or empty. This is applicable for exacloud PluginType script.",
            "  Example:",
            "    patchmetadata registerpluginscript ScriptName=syslens.sh ScriptBundleHash=754cc5fc57ac2060e7208b6403ae93302cb5d36cbaefbd7cfce19e16ae2c5c53 ScriptAlias=syslens_upgrade_script ChangeRequestId=CRID-1234 Description=syslens.sh upgrades syslens rpm on the dom0 node.This script is owned DB Cloud Frameworks team which develops Syslens/Applin PluginType=oneoff PluginTarget=dom0"
        ],
        "listpluginscripts": [
            "\n  Usage:",
            "  PluginTarget=<Name of the PluginTarget dom0|cell> PluginType=<PluginType exacloud|oneff> ScriptAlias=<Alias name for the script to be executed>",
            "  Description:Get the registered plugin scripts",
            "  Arguments:",
            "    PluginTarget: PluginTarget determines where the plugins are going to be executed, it can be dom0 or cell",
            "    PluginType: Execution context of the script. It can be exacloud or oneoff.",
            "    ScriptAlias: Represents simplified or alternative form of the script name. This alias name is used for modifying or deleting script metadata.",
            "  Example:",
            "    patchmetadata listpluginscripts PluginTarget=dom0 PluginType=oneoff ScriptAlias=syslens_upgrade_script",
            "    patchmetadata listpluginscripts PluginTarget=dom0 PluginType=exacloud",
            "    patchmetadata listpluginscripts"
        ],
        "updatepluginscript": [
            "\n  Usage:",
            "  PluginTarget=<Name of the PluginTarget dom0|cell> PluginType=<PluginType exacloud|oneff> ScriptAlias=<Alias name for the script to be executed>  ScriptName=<Script Name> ScriptBundleHash=<ScriptBundle Checksum>  ChangeRequestId=<ChangeRequestId> Description-<Brief description about the script>",
            "  Description: Update metadata of a registered plugin script for a particular PluginTarget with a specific PluginType having a specific ScriptAlias",
            "  Arguments:",
            "    PluginTarget: PluginTarget determines where the plugins are going to be executed, it can be dom0 or cell. This is mandatory parameter.",
            "    PluginType: Execution context of the script. It can be exacloud or oneoff. This is mandatory parameter.",
            "    ScriptAlias: Represents simplified or alternative form of the script name. This alias name is used for modifying or deleting script metadata. This is mandatory parameter.",
            "    ScriptName: Name of the script to be executed which include relative path from standard exacloud plugin scipts location.",
            "    ScriptBundleHash: The SHA256 hash of the tarball that contains the script, associated artifacts, and a checksum file.",
            "    ChangeRequestId: The ChangeRequestID created for the process of registering the script.",
            "    Description: This is to provide comprehensive details about the script, including a brief description of its functionality. Additionally, include Ownership details to capture information about the script???s owner details such as component owner or team distribution list(DL).",
            "    IsEnabled: This is to determine whether the script still required to be executed in the environment, with this attribute we can temporarily disable a script to be executed. Default value is Yes.",
            "    FailOnError: This is to determine whether to stop infra patching flow in case of script execution failure. Default value is Yes.",
            "    Phase: Refers to the phase during which the script is executed. It can be pre, post or empty. This is applicable for exacloud PluginType script.",
            "  Example:",
            "    patchmetadata updatepluginscript ScriptAlias=syslens_upgrade_script PluginType=oneoff PluginTarget=dom0 IsEnabled=No FailOnError=Yes"
        ],
        "deregisterpluginscript": [
            "\n  Usage:",
            "  PluginTarget=<Name of the PluginTarget dom0|cell> PluginType=<PluginType exacloud|oneff> ScriptAlias=<Alias name for the script to be executed>",
            "  Description:Deregister metadata of a registered plugin script",
            "    PluginTarget: PluginTarget determines where the plugins are going to be executed, it can be dom0 or cell",
            "    PluginType: Execution context of the script. It can be exacloud or oneoff.",
            "    ScriptAlias: Represents simplified or alternative form of the script name. This alias name is used for modifying or deleting script metadata.",
            "  Example:",
            "    patchmetadata deregisterpluginscript PluginTarget=dom0 PluginType=oneoff ScriptAlias=syslens_upgrade_script"
        ],
        "deletelaunchnodemetadata": [
            "\n  Usage:",
            "  LaunchNode=<Name of the launchNode> RequestId=<ECRA request ID>",
            "  Description:Delete launchNode metadata about running patch operations",
            "    LaunchNode: Name of the launchNode.",
            "    RequestId: ECRA request ID.",
            "  Example:",
            "    patchmetadata deletelaunchnodemetadata LaunchNode=slcs27adm0304.us.oracle.com",
            "    patchmetadata deletelaunchnodemetadata RequestId=e1443b1c-4113-11f0-b643-00001701ee8d",
            "    patchmetadata deletelaunchnodemetadata RequestId=e1443b1c-4113-11f0-b643-00001701ee8c LaunchNode=slcs27adm0304.us.oracle.com"
        ],
        "syncvmstate": [
            "\n  Usage:",
            "  rackName=<Name of rack> vmCustomerHostName=<Customer Hostname of VM>  vmState=<State of the VM like stopped/running. optional parameter>",
            "  Description: Updates VM state metadata(stopped/running) in ECRA. If vmState is specified, will only update ECRA metadata. If vmState is not specified, will sync vmState between ECRA and Exacloud",
            "  Arguments:",
            "    rackName: Name of the Rack",
            "    vmCustomerHostName: Customer Hostname of the VM for which VM state (stopped/running) needs to be synchronized",
            "    vmState: Optional parameter specifying vm state like running/stopped",
            "  Example:",
            "    patchmetadata syncvmstate rackName=slcs27 vmCustomerHostName=auto-hbzvr1.client.infrapatchdev.oraclevcn.com",
            "    patchmetadata syncvmstate rackName=slcs27 vmCustomerHostName=auto-hbzvr1.client.infrapatchdev.oraclevcn.com vmState=stopped"
        ],
        "syncinfraimages": [
            "\n  Usage:",
            "  rackName=<Name of rack> targetType=<targetType like dom0 or cell optional parameter>",
            "  Description: Sync node images version for infra components (dom0,cell) between Exacloud and ECRA",
            "  Arguments:",
            "    rackName: Name of the Rack",
            "    targetType: Optional parameter specifying targetType like dom0 or cell",
            "  Example:",
            "    patchmetadata syncinfraimages rackName=slcs27 ",
            "    patchmetadata syncvmstate rackName=slcs27 targetType=dom0"
        ]
    },
    "exaversion": {
        "patchesreport": " status=<status> [rackName=<rackName>] | [cabinetName=<cabinetName>] [imageVersion]\n Get patches report filtering by status=[FAILED | SUCCESS]\n Optional filter by rackName or cabinetName, and imageVersion\n  Example:\n    exaversion patchesreport status=SUCCESS\n    exaversion patchesreport rackName=<rackName> status=SUCCESS imageVersion=xxxxx\n    exaversion patchesreport rackName=<rackName> status=FAILED imageVersion=xxxxx\n    exaversion patchesreport cabinetName=<cabinetName> status=SUCCESS imageVersion=xxxxx\n    exaversion patchesreport cabinetName=<cabinetName> status=FAILED imageVersion=xxxxx\n    exaversion patchesreport status=Suceeded operation=rollback \n    exaversion patchesreport status=Suceeded patchType=MONTHLY",
        "register": [
            "\n  Usage:",
            "    serviceType=<To see all the valid service types please use the command in ecracli: properties get SERVICE_TYPE> [exasplice=<yes|no>] targetTypes=<dom0|cell|ibswitch|domu|dom0+cell> imageVersion=<version> [comments=<comments>] \\n Register exaversion with ECRA",
            "  Description:",
            "    Register exaversion for infra patching",
            "  Arguments:",
            "    serviceType: To see all the valid service types please use the command in ecracli: properties get SERVICE_TYPE",
            "    exasplice: Rebootless exadata bundle for monthly infrapaching",
            "    targetTypes: dom0 and cell or both dom0 and cell can be specified together for exasplice=yes. ibswitch and domu are not applicable for exasplice/monthly patching",
            "    rackModel: Rack Model if applicable. Sample values = X6-2, X7-2, X8-2, X8M-2, X9M-2, X10M-2, X10M-2L, X10M-2XL",
            "    imageVersion: Mapping to the LATEST version flag passed by the Control Plane",
            "    comments: for any additional user comments",
            "  Example:",
            "    exaversion register serviceType=EXACS exasplice=yes targetTypes=dom0 imageVersion=201026",
            "    exaversion register serviceType=EXACC targetTypes=dom0+cell imageVersion=19.3.6.0.0.200317",
            "    exaversion register serviceType=EXACC targetTypes=cps imageVersion=20.1.2.0.0.200930",
            "    exaversion register serviceType=EXACS exasplice=no rackModel=X8-2 targetTypes=dom0+cell imageVersion=23.1.5.0.0.230818",
            "    exaversion register serviceType=EXACS exasplice=yes targetTypes=dom0 rackModel=X8-2 imageVersion=201026",
            "    exaversion register serviceType=EXACS exasplice=no targetTypes=dom0+cell imageVersion=19.3.6.0.0.200317",
            "    exaversion register serviceType=EXACOMPUTE exasplice=yes targetTypes=dom0 imageVersion=201026",
            "    exaversion register serviceType=EXACOMPUTE targetTypes=dom0 imageVersion=19.3.6.0.0.200317"
        ],
        "deregisterexaversion": [
            "\n  Usage:",
            "    serviceType=<To see all the valid service types please use the command in ecracli: properties get SERVICE_TYPE> targetType=<dom0|cell|ibswitch> [exasplice=<yes|no>] [rackModel=<rackModel name>]  \\n Deregister exaversion with ECRA",
            "  Description:",
            "    Deregister exaversion for infra patching for a specific rackModel",
            "  Arguments:",
            "    serviceType(Mandatory): To see all the valid service types please use the command in ecracli: properties get SERVICE_TYPE",
            "    targetType(Mandatory): TargetType for which  imageversion is registered.Possible values are dom0 or cell or ibswitch",
            "    rackModel(Mandatory): Rack Model name. Sample values = X6-2, X7-2, X8-2, X8M-2, X9M-2, X10M-2, X10M-2L, X10M-2XL",
            "    exasplice(Optional): Rebootless exadata bundle for monthly infrapaching. Default value is no",
            "  Example:",
            "    exaversion deregisterexaversion serviceType=EXACS targetType=dom0 rackModel=X8-2",
            "    exaversion deregisterexaversion serviceType=EXACS targetType=cell rackModel=X8-2",
            "    exaversion deregisterexaversion serviceType=EXACS exasplice=no rackModel=X8-2 targetType=dom0",
            "    exaversion deregisterexaversion serviceType=EXACC exasplice=yes targetType=dom0 rackModel=X8-2"
        ],
        "registerimageseries": [
            "\n  Usage:",
            "    serviceType=<To see all the valid service types please use the command in ecracli: properties get SERVICE_TYPE> [exasplice=<yes|no>] targetTypes=<dom0|cell|dom0+cell> seriesLabel=<like 23.x> maxVersion=<like 23.3.6.0.0.200317> [comments=<comments>] \\n Register exaversion image series with ECRA",
            "  Description:",
            "    Register max Image Version for a specific series like 23.x for infra patching",
            "  Arguments:",
            "    serviceType: To see all the valid service types please use the command in ecracli: properties get SERVICE_TYPE",
            "    exasplice: Monthly Patching operation",
            "    targetTypes: dom0 and cell or both dom0 and cell can be specified together for exasplice=yes ",
            "    seriesLabel: Label for a specific series like 24.x",
            "    maxVersion: Maximum version in the seriesLabel",
            "    comments: for any additional user comments",
            "  Example:",
            "    exaversion registerimageseries serviceType=FA exasplice=yes targetTypes=cell seriesLabel=23.x maxVersion=23.3.6.0.0.200317",
            "    exaversion registerimageseries serviceType=EXACS exasplice=yes targetTypes=cell seriesLabel=24.x maxVersion=24.3.6.0.0.200317"
        ],
        "getimageseries": [
            "\n Usage:",
            "    exaversion getimageseries",
            "    exaversion getimageseries targetType=CELL exasplice=yes serviceType=EXACS seriesLabel=24.x",
            "    Sample Output:\n",
            "    {\"InfraPatchImagesSeries\": [{\"serviceType\": \"FA\", \"patchType\": \"MONTHLY\", \"targetType\": \"CELL\", \"series\":\"24.x\",\"maxVersion\": \"24.3.6.0.0.200317\"}",
            "    {\"status\": 200, \"op\": \"patch_imageseries_get\", \"status-detail\": \"success\"}"
        ],
        "get": [
            "\n List all patch exaversions in ECRA.",
            "  Usage:",
            "    exaversion get",
            "    exaversion get targetType=DOM0",
            "    exaversion get targetType=DOM0 exasplice=yes serviceType=EXACS",
            "    exaversion get targetType=CELL exasplice=no serviceType=EXACS",
            "    exaversion get serviceType=ADBD",
            "    exaversion get exasplice=yes",
            "    Sample Output:\n",
            "    {\"ExaVersions\": [{\"serviceType\": \"FA\", \"patchType\": \"QUARTERLY\", \"targetType\": \"DOM0\", \"imageVersion\": \"19.3.6.0.0.200317\"}",
            "    {\"serviceType\": \"FA\", \"patchType\": \"QUARTERLY\", \"targetType\": \"CELL\", \"imageVersion\": \"19.3.6.0.0.200317\"}",
            "    {\"serviceType\": \"FA\", \"patchType\": \"MONTHLY\", \"targetType\": \"DOM0\", \"imageVersion\": \"201026\"}",
            "    {\"status\": 200, \"op\": \"exa_version_list\", \"status-detail\": \"success\"}"
        ],
        "listpatches": [
            "\n  Display and Validate the physical location of the exadata patches in the payloads directory",
            "    Usage:",
            "    exaversion listpatches",
            "    exaversion listpatches patchId=<imageVersion>",
            "    exaversion listpatches exaocid=<exaocid>",
            "    exaversion listpatches exaocid=<exaocid> patchId=<imageVersion>",
            "    exaversion listpatches exaocid=<exaocid> targetType=cps",
            "  Sample Output: \n",
            " {\"patchlist\": [\"20.1.3.0.0.201023\", \"201027\"], \"status\": 200, \"op\": \"exa_version_list_patch_from_path\", \"status-detail\": \"success\"}"
        ],
        "listpatchesmetadata": [
            "\n  Display metadata of all exadata patches in the payloads directory",
            "    Usage:",
            "    exaversion listpatchesmetadata exaocid=<exaocid> targetTypes=<targets like dom0 etc> serviceType=<serviceType> patchType=<patchType>",
            "    Usage:",
            "    exaversion listpatchesmetadata exaocid=<exaocid> targetTypes=dom0 serviceType=EXACC patchType=QUARTERLY",
            "    exaversion listpatchesmetadata exaocid=<exaocid> targetTypes=dom0+cell serviceType=EXACC patchType=QUARTERLY",
            "    exaversion listpatchesmetadata exaocid=<exaocid> targetTypes=dom0 serviceType=EXACC patchType=MONTHLY",
            "    Sample Output: \n",
            "    {\"patchlist\":[{\"imageVersion\":\"21.2.6.0.0.211112\",\"patchType\":\"QUARTERLY\",\"targetType\":\"dom0\",\"serviceType\":\"EXACC\",\"bpDate\":\"211112\",\"bpName\":\"21.2.6.0.0\"},",
            "    {\"imageVersion\":\"21.2.8.0.0.211012\",\"patchType\":\"QUARTERLY\",\"targetType\":\"dom0\",\"serviceType\":\"EXACC\",\"bpDate\":\"211012\",\"bpName\":\"21.2.8.0.0\"}],",
            "    \"status\":200,\"op\":\"exa_version_list_patch_from_metadata\",\"status-detail\":\"success\"}"
        ],
        "purge": [
            "\n  Deletes unwanted patches in Payloads directory to clean up disk space. Either patchVersion or retention can be specified as parameter.",
            "    patchVersion specifies the specific patch version to be deleted.",
            "    Retention parameter specifies the number of patches to retain across each category of patches - Quarterly, Dom0 Exasplice and Cell Exasplice",
            "    Usage:",
            "    exaversion purge patchVersion=20.1.13.0.0.210909",
            "    Sample Output",
            "    {\"purgedPayloads\":\"20.1.13.0.0.210909\",\"status\":200,\"op\":\"exa_version_purge\",\"status-detail\":\"success\"}\n",
            "    exaversion purge retention=<number of patches to retain in integer. If not specified, default value is 4>",
            "    Sample Output where purge payloads are separated by comma ",
            "    {\"purgedPayloads\":\"20.1.13.0.0.210909, 20.1.12.0.0.210909\",\"status\":200,\"op\":\"exa_version_purge\",\"status-detail\":\"success\"}"
        ]
    },
    "exacompute": {
        "ports": "\n  hostname=<HOSTNAME> the value should be the oracle hostname. \n  hwid=<HW_ID> the value should be the ID related to the oracle hostname.\n  smartnicid=<SMARTNIC_ID> the value should be the cavium id you are looking for.\nThis API will return all the SMARTNIC info related to the hostname or hw_id or smartnic_id provided",
        "getfleetstatelock": "[stateid=id] \n Acquires the fleet state lock handle and state handle if its available\n",
        "fleetstateunlock": "<fleetstatehandle=statehandle> <fleetlockhandle=lockhandle>\n Releases the lock \n",
        "getfleetstate": "<fleetstatehandle=statehandle>\n Get the fleet state corresponding to the state handle\n",
        "getfleetstateid": "\n Get the latest fleet state ids\n",
        "getlatestfleet": "\n Get the latest fleet state\n",
        "updatefleetstate": "<json_path=<file_path>> | <fleetLockHandle=lockhandle> <fleetStateHandle=statehandle> [fleetInfrastructure=json][plan=placement plan] [stateid=id]\n Updates the state store\n",
        "precheck": "hostname=<hostname>",
        "addcluster": "json_path=<payload path>. [hostnames=<HOSTNAMES>]",
        "addclusterprecheck": "json_path=<payload path>. [hostnames=<HOSTNAMES>]",
        "deletecluster": "vmclusterid=<VMCLUSTERID>",
        "listcluster": "[vmclusterid=<VMCLUSTERID> type=<TYPE> verbose=<true/false>] if the parameter is not provided, the command will show a list with all the exacompute racks. if vmcluster is provided will return the ECRA JSON. if type=microservice, JSON from microservice will be returned. If verbose is true, then an extensive report of the clusters will be given",
        "activecard": "oraclehostname=<ORACLEHOSTNAME> The API will return the active cavium for the provided oracle hostname",
        "vm": "[action='moveSanityCheck|movePrepare|move' exaUnitId=<VM's Cluster ExaUnitId> vmName=<VM's internal name> sourceDom0=<source DOM0> targetDom0=<target DOM0>] This API will execute ExaCompute VM actions on a particular VM\n",
        "createmdcontext": "<json_path=<payload path>>\n Creates MdContext for the QFAB\n",
        "deletemdcontext": "<fabricname=qfabname> \n Deletes MdContext for the QFAB\n",
        "updatemdcontext": "<fabricname=qfabname> <json_path=<payload path>> \nUpdates the details of a particular MDContext\n",
        "getmdcontext": "<fabricname=qfabname> \n Gets the details of a particular MDContext\n",
        "createmaintenancedomain": "<json_path=<payload path>>\n Creates a maintenance Domain in a QFAB\n",
        "deletemaintenancedomain": "<fabricname=qfabname> <maintenancedomainid=mdid>\n Deletes a maintenance domain in a QFAB\n",
        "updatemaintenancedomain": "<fabricname=qfabname> <maintenancedomainid=mdid> <json_path=<payload path>> [action=start/stop] \nUpdates the details of a particular MD\n",
        "getmdnodes": "<fabricname=qfabname> <maintenancedomainid=mdid> \nGets the nodes in an MD\n",
        "listmaintenancedomain": "<fabricname=qfabname>\n  Lists all the MDs in the QFAB\n",
        "getmaintenancedomain": "<fabricname=qfabname> <maintenancedomainid=mdid>\n Gets the details of an MD\n",
        "updatenodemdmapping": "<json_path=<payload path>>\n Creates Node to maintenance domain ID mapping for the given node list \n",
        "computedetail": "hostname=<HOSTNAME> the value could be the oracle hostname or some smartnicid. Returns the list of racks associated with the provided parameter, this could be an oracle hostname or a smartnicid.\n",
        "reshapecluster": "vmclusterid=<VMCLUSTERID> [cores=<CORES> memoryGb=<MEMORY> ohomeSizeGb=<OHSIZE> filesystem=<[montpoint:size, etc.] volumes=<[volume:size, etc.]>]\n VMCLUSTERID: VmCluster OCID of the rack, CORES : #of Cores, MEMORY : Memory in GB, OHSIZE: Oracle home in GB Note: The provided values should be provided per node, and this quantity will be increased per node for all the nodes that belong to the exaunit provided. FILESYSTEM: For filesystem reshape is required a comma separated array of [name of the mountpoint: size in GB]. e.g. filesystem=/u01:50,/var:15 VOLUMES: For filesystem reshape in exacompute is also required a required a comma separated array of [name of the volume type: size in GB]. e.g. volumes=u01:50,system:15",
        "reshapecluster_precheck": "vmclusterid=<VMCLUSTERID> [cores=<CORES> memoryGb=<MEMORY> ohomeSizeGb=<OHSIZE>]\n VMCLUSTERID: VmCluster OCID of the rack, CORES : #of Cores, MEMORY : Memory in GB, OHSIZE: Oracle home in GB Note: The provided values should be provided per node, and this quantity will be increased per node for all the nodes that belong to the exaunit provided.",
        "initiator": "nodefqdn=<HOSTNAME> this value could be the hostname with or without the domain.\n Request initiator to Exacloud for the given node",
        "updateocid": "nodefqdn=<HOSTNAME> This value could be the hostname with or without the domain. nodeocid=<OCID> ocid of the hostname\n Update the node ocid for the provided hostname",
        "gettemplate": "type=<TYPE> [vmclusterocid=<VMCLUSTEROCID>]. Based on the provided parameters, command will return a template.",
        "postvolumes": "jsonpath=<JSONPATH>. Stores volumes information in ecra DB.",
        "getvolumes": "rackname=<RACKNAME> hostname=<dom0 hostname> [guestname=<Client hostname for domu> edvvolume=<volume device path>\n The rackname could be the name of the rack or the ocid of the rack. This API returns the volumes associated with the provided parameter",
        "validatevolumes": "rackname=<RACKNAME> [guestname=<domu guestname> edvvolume=<list of volume paths>]\nAlternative use with dom0 hostname:\nhostname=<dom0 hostname> [edvvolume=<list of volume paths>]\nYou should use rack or hostname but not both as they are not compatible",
        "generatesshkeys": "hostname=<dom0>\n Generates a VaultAccess ssh pair of keys for the indicated dom0",
        "getpublickey": "hostname=<dom0>\n Retreives the VaultAccess ssh public key for the indicated dom0",
        "getvaultaccess": "vaultaccessid=<VAULTACCESSID>\n Retreives the VaultAccess information for the indicated OCID",
        "updatevaultaccessdetails": "jsonpath=<Path of json file where the parameters are stored.> \n Generates and gets the VaultAccess details. Inside the json file must be the following details: hostname, exarootuser, exarooturl, vaultaccess, vaulti, trustcertificates",
        "deletevaultaccessdetails": "[hostname=<dom0> | vaultaccessid=<OCID>] \n Deletes the VaultAccess details for the indicated dom0 or Vault Access OCID",
        "precheckedvvolumes": "jsonpath=<JSONPATH>. Performs EDV volumeMountNames Precheck",
        "getnathostnames": "hostname=<HOSTNAME> the value could be the oracle hostname or some smartnicid. Returns the list of nat ips associated with the provided parameter, this could be an oracle hostname or a smartnicid.\n",
        "snapshotmount": "jsonpath=<JSONPATH> idemtoken=<IDEMTOKEN> vminstanceid=<VMINSTANCEID>\n Sends information to exacloud for vm snapshot mount operation",
        "snapshotunmount": "jsonpath=<JSONPATH> idemtoken=<IDEMTOKEN> vminstanceid=<VMINSTANCEID>\n Sends information to exacloud for vm snapshot unmount operation",
        "listsystemvault": "vaultid=<vaultId>\n List system vaults. vaultid option lists system vault with given vaultId",
        "updatesystemvault": "vaultid=<vaultId> jsonpath=<Path of json file where payload is stored> \n Updates system vault associated with given vaultId with the values from payload",
        "deletesystemvault": "vaultid=<vaultId>\n Delete system vault associated with given vaultId",
        "createsystemvault": "jsonpath=<Path of json file where payload is stored>\n Register API for system vault",
        "updatefleetnode": "hostname=<oracle_hostname> jsonpath=<Path of json file where payload is stored> \n Async workflow based API which performs metadata update on HardwareNodes ecs_hw_nodes table and addition/deletion/updation of node from fleet state-store",
        "clusterdetail": "vmclusterid=<VMCLUSTERID>. Returns the detailed information about vm instances in the cluster",
        "securevms": "vmclusterid=<VMCLUSTERID> [payload=json_path] [idemtoken=idemtoken]. Remove access keys for provided vm\n",
        "removenodexml": "rackname=<RACKNAME> dom0=<Dom0>. Removes information in the rack updated XML, related to the provided dom0.\n",
        "configureroceips": "hostname=<dom0name> [idemtoken=token] \n Configures RoCE IPs on the KVM hosts.\n",
        "deconfigureroceips": "hostname=<dom0name> [idemtoken=token] \n Deconfigures RoCE IPs on the KVM hosts.\n",
        "cleanuprequest": "requestid=<requestid> failedhost=<dom0 with hardware failure> This aborts the workflow, removes the failed host from xml, updates failed hosts and error information for status response\n",
        "nodedetail": "[page=<page> pagesize=<page size> servicetype=<exacompute>]\nGet Node Detail for all kvm host on the exacompute service.\n If servicetype is not provided then by default servicetype is exacompute",
        "clusterhistory": "rackname=<RACKNAME>\nUsed to get information about deleted clusters only.",
        "computecleanup": "hostname=<HOSTNAME>\nClean dom0",
        "updatefabricfleet": "fabricname=<FABRICID> operation=<OPERATION> [cloudvendor=<name> cloudproviderregion=<region> cloudproviderbuilding=<building> cloudprovideraz=<az> sitegroup=<sitegroup name>] \n FABRICID: Name of the fabric. OPERATION: add or delete\n",
        "dbvolumes": "operation<attach|detach|resize> vmclusterocid=<VM CLUSTER OCID> jsonpath=<PATH>\nExecute dbvolume operation on the given vm cluster ocid.",
        "runfleetjsoncheck": "\nExecute metadata sanity check on all dom0s and VM for the fleet state data",
        "getdomus": "\n hostname=<HOSTNAME FROM WHICH RETREIVE THE DOMUS>. \nGets a report that shows each DomU that has the Dom0 and how many are form each type.",
        "getvmclusterdetails": "\n vmclusterocid=<VMCLUSTEROCID> \nGets the details of thevmcluster that is indicated.",
        "rackreserve": "\n vmclusterid=<cluster id> hostnames=<Comma separated hostnames> [idemtoken=<idemtoken>]\nReserves a rack with specified hostnames for exacompute.",
        "guestreserve": "\n vmclusterid=<cluster id> hostnames=<Comma separated hostnames> [idemtoken=<idemtoken>]\nReserves a guest with specified hostnames for the rack.",
        "guestrelease": "\n vmclusterid=<cluster id> hostnames=<Comma separated hostnames> [idemtoken=<idemtoken>]\nReleases a guest with specified hostnames for the rack."
    },
    "sla": {
        "get_average_all": "[measure_from=<FROM_TIME>] [measure_to=<TO_TIME>]\nShows the average SLA of all racks for the given time range.\nDefault measure_to is now, and default measure_from is (measure_from-30days)\nFormat of measure_from/to is ISO8601, i.e. YYYY-MM-DDThh:mm:ssZ",
        "get_average": "rackname=<RACKNAME> [measure_from=<FROM_TIME>] [measure_to=<TO_TIME>]\nShows the average SLA of the rack for the given time range.\nDefault measure_to is now, and default measure_from is (measure_from-30days)\nFormat of measure_from/to is ISO8601, i.e. YYYY-MM-DDThh:mm:ssZ",
        "get_average_tenancy": "tenancy_ocid=<TENANCY_OCID> [measure_from=<FROM_TIME>] [measure_to=<TO_TIME>]\nShows the average SLA of the tenancy for the given time range.\nDefault measure_to is now, and default measure_from is (measure_from-30days)\nFormat of measure_from/to is ISO8601, i.e. YYYY-MM-DDThh:mm:ssZ",
        "get_details": "rackname=<RACKNAME> measure_from=<FROM_TIME> measure_to=<TO_TIME>\nShows raw SLA data records (timestamp and status)\nFormat of measure_from/to is ISO8601, i.e. YYYY-MM-DDThh:mm:ssZ",
        "get_tenant_report": "tenancyOcid=<TENANCY_OCID> [measureFrom=<FROM_TIME>] [measureTo=<TO_TIME>]\nShows the SLA report for a given time range for all the vm clsuters inside the given tenancy.\nDefault measure_to is now, and default measure_from is (measure_from-30days)\nFormat of measure_from/to is ISO8601, i.e. YYYY-MM-DDThh:mm:ssZ",
        "get_cei_report": "cei=<CEI> [measureFrom=<FROM_TIME>] [measureTo=<TO_TIME>]\nShows the SLA report for a given time range for all the vm clsuters inside the given cei.\nDefault measure_to is now, and default measure_from is (measure_from-30days)\nFormat of measure_from/to is ISO8601, i.e. YYYY-MM-DDThh:mm:ssZ",
        "get_vm_report": "vm=<VM_CLUSTER_OCID> [measureFrom=<FROM_TIME>] [measureTo=<TO_TIME>]\nShows the SLA report for a given time range for the vm clsuter given.\nDefault measure_to is now, and default measure_from is (measure_from-30days)\nFormat of measure_from/to is ISO8601, i.e. YYYY-MM-DDThh:mm:ssZ"
    },
    "topology": {
        "get_topology_for_ad": "ad=<AD_NAME> service=EXACS [rackname=<ECRA_CABINET_NAME>] [clustername=<VM_CLUSTER_INTERNAL_NAME] [switchname=<NETWORK_SWITCH_NAME>] [vmclusterocid=<VM_CLUSTER_OCID>]\nShows the topology data for an AD for ExaCS service. Result can be filtered based on the optional parameters provided.\nDefault value for all optional parameter is null. If no optional parameter provided, the result for entire ad provied will be returned.\n",
        "get_network_switch_for_ad": "ad=<AD_NAME> [physicalRackNumber=<PHYSICAL_RACK_NUMBER>]\nShows the network switches for an given ad. Results can be filtered based on the rack's physical rack number.\nDefault value for physicalRackNumber is null, all network switches in the ad will be returned."
    },
    "agent": {
        "request": "method=<METHOD> url=<URL> domu=<one/all> exaunit=<EXAUNIT> payload=<PAYLOAD> token=<TOKEN>\nFlexible request to communicate with ExaAgent.\nThe URL,payload,token and http method parameters are related to Agent services.",
        "dataplanesettings": "payload=<PAYLOAD>\nV2 Customer Opt-in for dataplane diagnostics (events, metrics, logs)\nThe flags in payload correspond to the UI buttons."
    },
    "cache": {
        "purge": "type=<TYPE> \n Purges the cache associated with the provided type. Available types [cspostchecks,csfilesystem,ecsproperties]"
    },
    "errorcode": {
        "get": "code=<CODE> \n Returns all the information for the given code.",
        "category": "code=<CODE>\n Returns the category description for the given code",
        "endpoint": "name=<NAME>\n Returns all the errors that a particular endpoint could have, in case you want to list all, use _all_ as value."
    },
    "preprov": {
        "scheduler": "<operation> Manages and retrieves the state of the preprov scheduler.",
        "get": "<job> Retrieves the details of a preprov job or a list of all the preprov jobs if no job_class is specified.",
        "update": "<job> configuration=<config_path> Updates a preprov job's metadata field using a pre-existing json file's contents.",
        "deleterackresources": "rackname=<rackname> [keepvnic=true|false] [keepcompute=true|false] [keepdns=true|false] Deletes all oci resources related to given rackname, unless keep options are used.",
        "capacitymove": "exadatainfrastructureid=<EXAINFRAID>.\nThis will delete all clusters if any and the oci resources attached to it, also will delete the infrastructure and return all hardware to CLUSTERTAG=ALL.",
        "vcn": {
            "get": "Retrieves a list with all the VCNs"
        },
        "subnet": {
            "get": "Retrieves a list with all the subnets"
        },
        "vnics": {
            "get": "[rackname=<rackname>]. Retrieves a list with all the vinics from specified rackname, if rackname is not specified all the vnics will be retrieved."
        }
    },
    "oci": {
        "connectivity check": "[compartmentid=<COMPARTMENT_ID>]\nThis will help to check if there is connectivity to oci apis",
        "compute create": "vnicid=<VNIC_ID> availabilitydomain=<AD> subnetid=<SUBNETID>  hostnamelabel=<HOSTNAMELABEL> displayname=<DISPLAYNAME> compartmentid=<COMPARTMENTID> shape=<SHAPE> serialnumber=<SERIALNUMBER> vlantag=<VLANTAG>\nCreates a new compute instance.",
        "compute delete": "instanceid=<INSTANCE_OCID>\n Deletes an instance",
        "compute get": "instanceid=<INSTANCE_OCID> [compartmentid=<COMPARTMENT_ID>]\n Gets an instance and displays all info",
        "compute createvnic": "availabilitydomain=<AD> subnetid=<SUBNETID>  hostnamelabel=<HOSTNAMELABEL> compartmentid=<COMPARTMENTID> isprimary=<PRIMARY VNIC>\nCreates a new vnic",
        "compute getvnic": "vnicid=<VNIC_OCID> [compartmentid=<COMPARTMENT_ID>]\nGet info for a vnic",
        "compute deletevnic": "vnicid=<VNIC_OCID>\nDeletes a vnic",
        "compute attachvnic": "instanceid=<COMPUTE_INSTANCE_OCID> vnicid=<VNIC_OCID> vlantag=<VLANTAG> macaddress=<MAC_ADDRESS>\nAttach a vnic to a compute instance",
        "compute detachvnic": "instanceid=<COMPUTE_INSTANCE_OCID> vnicdattachmentid=<VNIC_ATTACHMENT_ID> [<preservevnic>=true|false] [compartmentid=<COMPARTMENT_ID>]\nDetaches a vnic from a compute instance",
        "compute createfloatingip": "hostnamelabel=<HOSTNAMELABEL> vnicid=<VNIC_OCID> [vlantag=<VLANTAG>]\nAutomatically creates a floating ip",
        "compute getFloatingip": "ipId=<ip ocid> \n Get a internal private IP",
        "compute updatefloatingip": "ipId=<ip ocid> [hostnameLabel=<hostname>] [lifetime=<RESERVED|EPHEMERAL>] \n Updates the specified private IP",
        "compute mapfloatingip": "ipId=<ip ocid> vnicid=<vnic ocid> \n map to the floating private IP to the given VNIC",
        "compute unmapFloatingip": "ipId=<ip ocid> \n Unmap the floating private IP from its current VNIC",
        "compute deletefloatingip": "floatingipid=<FLOATINGIP_ID>\nDeletes a floating ip",
        "compute creatednsrecord": "<not available>",
        "compute deletednsrecord": "<not available>",
        "compute listvnics": "compartmentid=<COMPARTMENT_ID>\nList all vnics for the provided compartment",
        "compute listips": "vnicid=<VNIC_ID>\nList all ips for the provided vnic"
    },
    "domu": {
        "get": "name=<DOMU_NAME> \nReturns all info of a given domu name",
        "search": "<query_param>=<value>\nSearch for all domus in the query search",
        "deletebadhostname": "nathostname=<NATHOSTNAME>\n Deletes the domu record associated with the provided nathostname and the badnathostname rack"
    },
    "ingestion": {
        "import": "action=<import_action> cabinetname=<cabinet name> cabinetjson=<cabinet json with hw node details> \nPerforms import of a Cabinet as a part of ingestion"
    },
    "sop": {
        "request": "jsonpayload=<JSON_PAYLOAD> [exaocid=<ExaCC ExaOcid>]\n Create Sop script request on given hardware. exaocid is only sent for ExaCC envs",
        "list": " [exaocid=<ExaCC ExaOcid>]\n List available SOP script that can run on HW as well with available versions. exaocid is only sent for ExaCC envs ",
        "cancel": "jsonpayload=<JSON_PAYLOAD> [exaocid=<ExaCC ExaOcid>]\n Cancel selected execution of SOP operation for given uuid. exaocid is only sent for ExaCC envs"
    },
    "sitegroup": {
        "list": "[<key>=<value>]* \n List of all registry of site group. Can be filtered by any combination of parameters",
        "add": "building=<building_id> name=<name of sitegroup> restricted=[N/Y] region=<SG region> ad=<ad_id> cloudvendor=<cloud vendor name> cloudproviderregion=<value> [cloudprovideraz=<value>]  [cloudproviderbuilding=<value>] [mtu=<value>] [farchildsite=<[N]/Y>] [overlaybridgeused=<[N]/Y>] [dbaastoolsrpm=<dbaasname>] [dbaastoolsrpmchecksum=<checksum>]\n Add new registry of site group",
        "update": "<name> [<key>=<value>]* \n Update certain allowed properties for a given sitegroup\n",
        "updaterpm": "cloudvendor=<value> dbaastoolsrpm=<name> [dbaastoolsrpmchecksum=<SHA256 value>]\n Update default RPM used into cloud vendor in case is not the default one\n",
        "configurefeature": "name=<NAME> feature=<ALL,VMBOSS> value=<ENABLED|DISABLED> \n Update configured features in the sitegroup\n"
    },
    "artifact": {
        "deliver": "artifact=<ARTIFACT_NAME> source=<PATH_TO_ARTIFACT_BINARIES> target=<TARGET_TYPE> [id=<TARGET_ID> targetname=<TARGET_NAME> model=<TARGET_MODEL> clustertag=<TARGET_CLUSTERTAG> type=<TARGET_TYPE>]\nIt will send the artifact binary to the specified target to be executed on, target can be a node and specify wich node using the optional properties, or it can be a rack, infrastructure, cabinet.",
        "deliver_status": "statusid=<Status id>.\n Used to fetch the status of the operation in full detail."
    },
    "compatibility": {
        "list": "[operationname=<operation name>].\n List the operations and their compatibility",
        "add": "operationname=<operation name> secondoperation=<operation name>.\n Add the compatibility between specified operations",
        "remove": "operationname=<operation name> secondoperation=<operation name>.\n Remove the compatibility between specified operations",
        "check": "operationname=<operation name> secondoperation=<operation name>.\n Check the compatibility between specified operations",
        "validoperations": "\n Retrieve the valid operations for compatibility"
    },
    "vault": {
        "details": "exaOcid=<Infrastructure exadata ocid> \nReturns vault details of a given exadata infra"
    },
    "faultinjection": {
        "listinfra": "\n Show all whitelisted infra for fault injection",
        "addinfra": "infraocid=<exa infra ocid>\n Add an infra to the fault injection whitelist",
        "deleteinfra": "infraocid=<exa infra ocid>\n Remove an infra from the fault injection whitelist"
    },
    "resourceblackout": {
        "getlatest": "resourcename=<resource name> [audit=<true|false>]\n Get latest blackout entry with given resource name",
        "gethistory": "resourcename=<resource name> [daylimit=<days in integer>]\n Get history of blackouts with given resource name",
        "getenabled": "\n Get list of enabled blackouts",
        "create": "payload=<path to payload>\n Create blackout",
        "update": "payload=<path to payload> resourcename=<resource name>\n Update blackout",
        "refresh": "resourcename=<resource name>\n Refresh status of blackout",
        "disable": "resourcename=<resource name>\n Sets blackout status to DISABLED"
    },
    "exascale": {
        "ersip register": "[ocid=<floating ip ocid> ip=<ip address> cabinet_name=<cabinet name> ip_state=<AVAILABLE|ASSIGNED>] | [flat_file=<path>]\n Register an ERS IP into the pool"
    }
}
